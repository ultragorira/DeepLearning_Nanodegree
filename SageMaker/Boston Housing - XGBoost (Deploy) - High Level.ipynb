{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Boston Housing Prices\n",
    "\n",
    "## Using XGBoost in SageMaker (Deploy)\n",
    "\n",
    "_Deep Learning Nanodegree Program | Deployment_\n",
    "\n",
    "---\n",
    "\n",
    "As an introduction to using SageMaker's High Level Python API we will look at a relatively simple problem. Namely, we will use the [Boston Housing Dataset](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) to predict the median value of a home in the area of Boston Mass.\n",
    "\n",
    "The documentation for the high level API can be found on the [ReadTheDocs page](http://sagemaker.readthedocs.io/en/latest/)\n",
    "\n",
    "## General Outline\n",
    "\n",
    "Typically, when using a notebook instance with SageMaker, you will proceed through the following steps. Of course, not every step will need to be done with each project. Also, there is quite a lot of room for variation in many of the steps, as you will see throughout these lessons.\n",
    "\n",
    "1. Download or otherwise retrieve the data.\n",
    "2. Process / Prepare the data.\n",
    "3. Upload the processed data to S3.\n",
    "4. Train a chosen model.\n",
    "5. Test the trained model (typically using a batch transform job).\n",
    "6. Deploy the trained model.\n",
    "7. Use the deployed model.\n",
    "\n",
    "In this notebook we will be skipping step 5, testing the model. We will still test the model but we will do so by first deploying the model and then sending the test data to the deployed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker==1.72.0\n",
      "  Downloading sagemaker-1.72.0.tar.gz (297 kB)\n",
      "     |████████████████████████████████| 297 kB 29.3 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: boto3>=1.14.12 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.21.42)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.19.5)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (3.17.2)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.5.3)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (0.1.5)\n",
      "Collecting smdebug-rulesconfig==0.1.4\n",
      "  Downloading smdebug_rulesconfig-0.1.4-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (4.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (21.3)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.25.0,>=1.24.42 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (1.24.42)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (0.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker==1.72.0) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker==1.72.0) (3.10.0.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging>=20.0->sagemaker==1.72.0) (2.4.7)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker==1.72.0) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.25.0,>=1.24.42->boto3>=1.14.12->sagemaker==1.72.0) (1.26.8)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.25.0,>=1.24.42->boto3>=1.14.12->sagemaker==1.72.0) (2.8.1)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-1.72.0-py2.py3-none-any.whl size=388327 sha256=5c47aeaa6c144f0121f02290d82cd4e58432bef1041107ffe6a52ab65c45da84\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/c3/58/70/85faf4437568bfaa4c419937569ba1fe54d44c5db42406bbd7\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: smdebug-rulesconfig, sagemaker\n",
      "  Attempting uninstall: smdebug-rulesconfig\n",
      "    Found existing installation: smdebug-rulesconfig 1.0.1\n",
      "    Uninstalling smdebug-rulesconfig-1.0.1:\n",
      "      Successfully uninstalled smdebug-rulesconfig-1.0.1\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.86.2\n",
      "    Uninstalling sagemaker-2.86.2:\n",
      "      Successfully uninstalled sagemaker-2.86.2\n",
      "Successfully installed sagemaker-1.72.0 smdebug-rulesconfig-0.1.4\n"
     ]
    }
   ],
   "source": [
    "# Make sure that we use SageMaker 1.x\n",
    "!pip install sagemaker==1.72.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Setting up the notebook\n",
    "\n",
    "We begin by setting up all of the necessary bits required to run our notebook. To start that means loading all of the Python modules we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the modules above, we need to import the various bits of SageMaker that we will be using. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "# This is an object that represents the SageMaker session that we are currently operating in. This\n",
    "# object contains some useful information that we will need to access later such as our region.\n",
    "session = sagemaker.Session()\n",
    "\n",
    "# This is an object that represents the IAM role that we are currently assigned. When we construct\n",
    "# and launch the training job later we will need to tell it what IAM role it should have. Since our\n",
    "# use case is relatively simple we will simply assign the training job the role we currently have.\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Downloading the data\n",
    "\n",
    "Fortunately, this dataset can be retrieved using sklearn and so this step is relatively straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Preparing and splitting the data\n",
    "\n",
    "Given that this is clean tabular data, we don't need to do any processing. However, we do need to split the rows in the dataset up into train, test and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we package up the input data and the target variable (the median value) as pandas dataframes. This\n",
    "# will make saving the data to a file a little easier later on.\n",
    "\n",
    "X_bos_pd = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "Y_bos_pd = pd.DataFrame(boston.target)\n",
    "\n",
    "# We split the dataset into 2/3 training and 1/3 testing sets.\n",
    "X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(X_bos_pd, Y_bos_pd, test_size=0.33)\n",
    "\n",
    "# Then we split the training set further into 2/3 training and 1/3 validation sets.\n",
    "X_train, X_val, Y_train, Y_val = sklearn.model_selection.train_test_split(X_train, Y_train, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Uploading the training and validation files to S3\n",
    "\n",
    "When a training job is constructed using SageMaker, a container is executed which performs the training operation. This container is given access to data that is stored in S3. This means that we need to upload the data we want to use for training to S3. We can use the SageMaker API to do this and hide some of the details.\n",
    "\n",
    "### Save the data locally\n",
    "\n",
    "First we need to create the train and validation csv files which we will then upload to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our local data directory. We need to make sure that it exists.\n",
    "data_dir = '../data/boston'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use pandas to save our train and validation data to csv files. Note that we make sure not to include header\n",
    "# information or an index as this is required by the built in algorithms provided by Amazon. Also, it is assumed\n",
    "# that the first entry in each row is the target variable.\n",
    "\n",
    "pd.concat([Y_val, X_val], axis=1).to_csv(os.path.join(data_dir, 'validation.csv'), header=False, index=False)\n",
    "pd.concat([Y_train, X_train], axis=1).to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to S3\n",
    "\n",
    "Since we are currently running inside of a SageMaker session, we can use the object which represents this session to upload our data to the 'default' S3 bucket. Note that it is good practice to provide a custom prefix (essentially an S3 folder) to make sure that you don't accidentally interfere with data uploaded from some other notebook or project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'boston-xgboost-deploy-hl'\n",
    "\n",
    "val_location = session.upload_data(os.path.join(data_dir, 'validation.csv'), key_prefix=prefix)\n",
    "train_location = session.upload_data(os.path.join(data_dir, 'train.csv'), key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train the XGBoost model\n",
    "\n",
    "Now that we have the training and validation data uploaded to S3, we can construct our XGBoost model and train it. We will be making use of the high level SageMaker API to do this which will make the resulting code a little easier to read at the cost of some flexibility.\n",
    "\n",
    "To construct an estimator, the object which we wish to train, we need to provide the location of a container which contains the training code. Since we are using a built in algorithm this container is provided by Amazon. However, the full name of the container is a bit lengthy and depends on the region that we are operating in. Fortunately, SageMaker provides a useful utility method called `get_image_uri` that constructs the image name for us.\n",
    "\n",
    "To use the `get_image_uri` method we need to provide it with our current region, which can be obtained from the session object, and the name of the algorithm we wish to use. In this notebook we will be using XGBoost however you could try another algorithm if you wish. The list of built in algorithms can be found in the list of [Common Parameters](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "There is a more up to date SageMaker XGBoost image. To use the newer image, please set 'repo_version'='1.0-1'. For example:\n",
      "\tget_image_uri(region, 'xgboost', '1.0-1').\n",
      "Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "# As stated above, we use this utility method to construct the image name for the training container.\n",
    "container = get_image_uri(session.boto_region_name, 'xgboost')\n",
    "\n",
    "# Now that we know which container to use, we can construct the estimator object.\n",
    "xgb = sagemaker.estimator.Estimator(container, # The name of the training container\n",
    "                                    role,      # The IAM role to use (our current role in this case)\n",
    "                                    train_instance_count=1, # The number of instances to use for training\n",
    "                                    train_instance_type='ml.m4.xlarge', # The type of instance ot use for training\n",
    "                                    output_path='s3://{}/{}/output'.format(session.default_bucket(), prefix),\n",
    "                                                                        # Where to save the output (the model artifacts)\n",
    "                                    sagemaker_session=session) # The current SageMaker session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before asking SageMaker to begin the training job, we should probably set any model specific hyperparameters. There are quite a few that can be set when using the XGBoost algorithm, below are just a few of them. If you would like to change the hyperparameters below or modify additional ones you can find additional information on the [XGBoost hyperparameter page](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        objective='reg:linear',\n",
    "                        early_stopping_rounds=10,\n",
    "                        num_round=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our estimator object completely set up, it is time to train it. To do this we make sure that SageMaker knows our input data is in csv format and then execute the `fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-24 18:32:03 Starting - Starting the training job...\n",
      "2022-04-24 18:32:29 Starting - Preparing the instances for training.........\n",
      "2022-04-24 18:33:57 Downloading - Downloading input data......\n",
      "2022-04-24 18:34:38 Training - Downloading the training image...\n",
      "2022-04-24 18:35:33 Training - Training image download completed. Training in progress..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[2022-04-24:18:35:37:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[34m[2022-04-24:18:35:37:INFO] File size need to be processed in the node: 0.02mb. Available memory size in the node: 8512.55mb\u001b[0m\n",
      "\u001b[34m[2022-04-24:18:35:37:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[18:35:37] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[18:35:37] 227x13 matrix with 2951 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[2022-04-24:18:35:37:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[18:35:37] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[18:35:37] 112x13 matrix with 1456 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[0]#011train-rmse:19.3925#011validation-rmse:19.3717\u001b[0m\n",
      "\u001b[34mMultiple eval metrics have been passed: 'validation-rmse' will be used for early stopping.\u001b[0m\n",
      "\u001b[34mWill train until validation-rmse hasn't improved in 10 rounds.\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[1]#011train-rmse:15.7995#011validation-rmse:15.9185\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[2]#011train-rmse:12.9221#011validation-rmse:13.1096\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[3]#011train-rmse:10.6322#011validation-rmse:11.0108\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[4]#011train-rmse:8.77312#011validation-rmse:9.25342\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 2 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[5]#011train-rmse:7.30602#011validation-rmse:7.87418\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[6]#011train-rmse:6.15034#011validation-rmse:6.81186\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[7]#011train-rmse:5.17588#011validation-rmse:5.93542\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[8]#011train-rmse:4.4717#011validation-rmse:5.30509\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[9]#011train-rmse:3.85154#011validation-rmse:4.8132\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10]#011train-rmse:3.37748#011validation-rmse:4.4476\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[11]#011train-rmse:3.02541#011validation-rmse:4.15951\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[12]#011train-rmse:2.72407#011validation-rmse:3.95406\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[13]#011train-rmse:2.49207#011validation-rmse:3.83217\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[14]#011train-rmse:2.30158#011validation-rmse:3.77505\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[15]#011train-rmse:2.16494#011validation-rmse:3.72631\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[16]#011train-rmse:2.064#011validation-rmse:3.70108\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17]#011train-rmse:1.99523#011validation-rmse:3.67181\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[18]#011train-rmse:1.91721#011validation-rmse:3.63175\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[19]#011train-rmse:1.85846#011validation-rmse:3.61689\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[20]#011train-rmse:1.75481#011validation-rmse:3.552\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[21]#011train-rmse:1.69316#011validation-rmse:3.51608\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22]#011train-rmse:1.6375#011validation-rmse:3.48036\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[23]#011train-rmse:1.58957#011validation-rmse:3.45321\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[24]#011train-rmse:1.5485#011validation-rmse:3.4757\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[25]#011train-rmse:1.51538#011validation-rmse:3.46083\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[26]#011train-rmse:1.47871#011validation-rmse:3.42928\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[27]#011train-rmse:1.43292#011validation-rmse:3.41991\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[28]#011train-rmse:1.36637#011validation-rmse:3.40628\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[29]#011train-rmse:1.33504#011validation-rmse:3.37952\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[30]#011train-rmse:1.31384#011validation-rmse:3.38643\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[31]#011train-rmse:1.28272#011validation-rmse:3.36605\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[32]#011train-rmse:1.24145#011validation-rmse:3.35871\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[33]#011train-rmse:1.20766#011validation-rmse:3.34939\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 6 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[34]#011train-rmse:1.20333#011validation-rmse:3.35379\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[35]#011train-rmse:1.18142#011validation-rmse:3.34185\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 8 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[36]#011train-rmse:1.16093#011validation-rmse:3.32943\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 10 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[37]#011train-rmse:1.14791#011validation-rmse:3.33034\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[38]#011train-rmse:1.12087#011validation-rmse:3.33853\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[39]#011train-rmse:1.08777#011validation-rmse:3.34113\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 6 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[40]#011train-rmse:1.07294#011validation-rmse:3.35861\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 14 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[41]#011train-rmse:1.0656#011validation-rmse:3.35783\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 10 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[42]#011train-rmse:1.05605#011validation-rmse:3.36274\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 10 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[43]#011train-rmse:1.03877#011validation-rmse:3.3546\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 10 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[44]#011train-rmse:1.02705#011validation-rmse:3.35186\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[45]#011train-rmse:1.00981#011validation-rmse:3.35255\u001b[0m\n",
      "\u001b[34m[18:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[46]#011train-rmse:0.992907#011validation-rmse:3.35468\u001b[0m\n",
      "\u001b[34mStopping. Best iteration:\u001b[0m\n",
      "\u001b[34m[36]#011train-rmse:1.16093#011validation-rmse:3.32943\u001b[0m\n",
      "\n",
      "2022-04-24 18:35:49 Uploading - Uploading generated training model\n",
      "2022-04-24 18:35:49 Completed - Training job completed\n",
      "Training seconds: 112\n",
      "Billable seconds: 112\n"
     ]
    }
   ],
   "source": [
    "# This is a wrapper around the location of our train and validation data, to make sure that SageMaker\n",
    "# knows our data is in csv format.\n",
    "s3_input_train = sagemaker.s3_input(s3_data=train_location, content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data=val_location, content_type='csv')\n",
    "\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test the trained model\n",
    "\n",
    "We will be skipping this step for now. We will still test our trained model but we are going to do it by using the deployed model, rather than setting up a batch transform job.\n",
    "\n",
    "\n",
    "## Step 6: Deploy the trained model\n",
    "\n",
    "Now that we have fit our model to the training data, using the validation data to avoid overfitting, we can deploy our model and test it. Deploying is very simple when we use the high level API, we need only call the `deploy` method of our trained estimator.\n",
    "\n",
    "**NOTE:** When deploying a model you are asking SageMaker to launch an compute instance that will wait for data to be sent to it. As a result, this compute instance will continue to run until *you* shut it down. This is important to know since the cost of a deployed endpoint depends on how long it has been running for.\n",
    "\n",
    "In other words **If you are no longer using a deployed endpoint, shut it down!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------!"
     ]
    }
   ],
   "source": [
    "xgb_predictor = xgb.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Use the model\n",
    "\n",
    "Now that our model is trained and deployed we can send the test data to it and evaluate the results. Here, because our test data is so small, we can send it all using a single call to our endpoint. If our test dataset was larger we would need to split it up and send the data in chunks, making sure to accumulate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to tell the endpoint what format the data we are sending is in\n",
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.serializer = csv_serializer\n",
    "\n",
    "Y_pred = xgb_predictor.predict(X_test.values).decode('utf-8')\n",
    "# predictions is currently a comma delimited string and so we would like to break it up\n",
    "# as a numpy array.\n",
    "Y_pred = np.fromstring(Y_pred, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how well our model works we can create a simple scatter plot between the predicted and actual values. If the model was completely accurate the resulting scatter plot would look like the line $x=y$. As we can see, our model seems to have done okay but there is room for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Median Price vs Predicted Price')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAofUlEQVR4nO3de5xcdX3/8dc7mwWW64IEjAsheANBkGiUSvqzgApWbhGLyE9bqP6ktloUKSUqv4JFHqY/WsXWtoqWiqIIrRBRq0gJeEFRExNEBB5qhcASIUgit1VC8vn9cc6Eyeyc2TM758xlz/v5eOxjZ86cOec7Z2c/853P96aIwMzMqmNWrwtgZmbd5cBvZlYxDvxmZhXjwG9mVjEO/GZmFePAb2ZWMQ78loukkPTc9PbHJf3fPijTmyR9o9fl6DZJn5b0wfT2/5J0V5fOu+U9UMCxbpd0eBHHsvY58M8wku6W9KSk3Ru2r07/ced3eo6IeHtEXNDpcRpJOlzSZkmPSXpU0l2S/rRFOT4XEUcVXY4ipH+HifS1PCDp3yXtWPR5IuLbEbFfjvKcJuk7RZ+/7vg3Sfpt+nofknS1pLlZ+0fEgRFxU1nlsdYc+GemXwKn1O5IOggY6V1x2nJ/ROwI7AycA3xS0gGNO0ma3fWSte+49LW8GHgpcG7jDgPyOvJ6Z/p6nw+MAh9p3GGGvd6B5cA/M30W+JO6+6cCn6nfQdK2kv5e0pq0RvpxSSN1j58taa2k+yW9peG59amGXSV9RdI6SevT23vV7XuTpAsk3ZzW4r/R+G2kmUgsA9YDB6Q11pslfUTSw8D5jbVYSQdKul7Sw+lrel+6fZakJZJ+IenXkq6StFuz80q6Q9KxdfdnpzXYF0vaTtLl6TE2SPqhpD1zvJZx4GvAC9NjhqR3SPoZ8LN027Hpt7INkr4r6eC6MiyQ9KP0+l0JbFf32OGS7qu7v3da216XlvNjkl4AfBx4eVoj35DuO+33wBSv92Hgi3Wv925J50j6MfB4ek3vlvSq9PEhSe9L/z6PSlopae/0sf3r/qZ3SXpD3nJYNgf+mekWYGdJL5A0BJwMXN6wz9+R1MwOAZ4LjAF/AyDpNcBfAa8Gnge8qsW5ZgH/DuwDzAMmgI817PO/gT8F9gC2SY/dUhqsX0dSc7wt3Xwo8D/pcS5s2H8n4L+BrwPPSl/TDenDZwCLgT9IH1sP/HPGqa+g7tsScDTwUET8iOQDdBdgb+AZwNvT1zvVa9kbeC2wqm7z4vT1HCDpxcClwJ+lx/0EcG0amLcBlpF8mO8G/Afw+ozzDAFfAe4B5pP8Tb8QEXekZf1eROwYEaPpU4p6DzSWY/e0jPWv9xTgGGA0Ip5qeMp70sdfS/JN7y3AE5J2AK4HPk/yNz8F+BdJB+Yti2WICP/MoB/gbpJ/0nOBDwGvIfnnmQ0ESUAQ8DjwnLrnvRz4ZXr7UmBp3WPPT5/73PT+p4EPZpz/EGB93f2bgHPr7v8F8PWM5x4ObAY2AA8Dq4E3po+dBqxp2P804Dvp7VOAVRnHvQN4Zd39ucBGYHaTfZ8LPApsn97/HPA36e23AN8FDs75d3gsfS33AP8CjKSPBXBk3b7/ClzQ8Py7SD6oXgHcD6juse/Wrn96ze6r+xuuy3hdW65Ver+j90CT498EPJG+3vH0us2puxZvafY+rXutJzQ55snAtxu2fQI4r9f/Z4P+43zbzPVZ4FvAvjSkeYA5wPbASkm1bQKG0tvPAlbW7X9P1kkkbU+Sy30NsGu6eSdJQxGxKb3/q7qnPAG0auS8PyL2ynjs3hbP2xv4RcZj+wDXSNpct20TsCdJkNoiIn4u6Q7gOElfBo4HFqQPfzY9zxckjZJ8i3p/RGzMOO/iiPjvHK9lH+BUSX9Zt20bkr9DAOORRr1U1t9jb+CemFyjbqaw90CdMyLiUxmPTedvtw9waC01lZpN8newDjjVM0NFxD0kjbyvBa5uePghkhTFgRExmv7sEknDHMBakn/GmnktTnUWsB9waETsTFJDhSSIFK3VVLL3As9p8dgf1r3W0YjYLpLcezO1dM8JwE8j4ucAEbExIj4QEQcAhwHHsnVbSjvqX8u9wIUN5ds+Iq4g+VuMqS46k/33uBeYp+YNqI3Xrsj3QB7T+dvdC3yz4brsGBF/3mFZKs+Bf2Z7K0lK4fH6jRGxGfgk8BFJewBIGpN0dLrLVcBpkg5Ia/TntTjHTiQBZEPaYNpq3zJ9BXimpHenufGdJB2aPvZx4EJJ+wBImiPphBbH+gJwFPDnJPll0ucdIemgNJf+CEm6aFPzQ7Tlk8DbJR2qxA6SjknbLb4HPAWckTaKngi8LOM4PyAJ2EvTY2wnaVH62APAXmmbQdHvgU59CrhA0vPS13+wpGeQ/E2fL+mPJQ2nPy9NG6utAw78M1hE/CIiVmQ8fA7wc+AWSY+QNIzulz7va8DFwPJ0n+UtTnMxSVfRh0galb9eRNnbFRGPkjREHkeSWvoZcET68EeBa4FvSHo0LeehzY6THmstScA9DLiy7qFnAv9JEvTvAL7J5Ebz6ZR9BfA2kkbx9STX/LT0sSeBE9P760ny3o3f4GrH2UTy+p8LrAHuS/eH5G94O/ArSQ+l24p6D3TqwyQfNN8gubb/RtIe8ijJB/AbSdo5fkXSIL1tiWWpBG2dOjQzs5nONX4zs4px4DczqxgHfjOzinHgNzOrmIEYwLX77rvH/Pnze10MM7OBsnLlyociYk7j9oEI/PPnz2fFiqxeiWZm1oykpiOuneoxM6sYB34zs4px4DczqxgHfjOzinHgNzOrmIHo1WNmVjXLVo1z0XV3cf+GCZ41OsLZR+/H4gVjhRzbgd/MrM8sWzXOe6++jYmNyazf4xsmeO/VyQqkRQR/p3rMzPrMRdfdtSXo10xs3MRF191VyPEd+M3M+sz4hom2trer1FSPpLtJFq7eBDwVEQvTVZquJFn0+27gDRGxvsxymJkNkiGJTU3WShlSMSuadqPGf0REHBIRC9P7S4AbIuJ5wA3pfTMzSzUL+q22t6sXqZ4TgMvS25cBi3tQBjOzvjU2OtLW9naVHfiDZJ3TlZJOT7ftma5pWlvbdI9mT5R0uqQVklasW7eu5GKamfWPs4/ej5Hhoa22jQwPcfbR+xVy/LK7cy6KiPsl7QFcL+nOvE+MiEuASwAWLlzohYHNrDJqXTYHsh9/RNyf/n5Q0jXAy4AHJM2NiLWS5gIPllkGM7NBtHjBWGGBvlFpqR5JO0jaqXYbOAr4CXAtcGq626nAl8oqg5mZTVZmjX9P4Bol3Y9mA5+PiK9L+iFwlaS3AmuAk0osg5mZNSgt8EfE/wAvarL918AryzqvmZm15pG7ZmYV48BvZlYxnp3TzDpW5hTCVjwHfjPrSNlTCFvxnOoxs46UPYWwFc+B38w6cn/GVMFZ2633nOoxs448a3Sk6TzxzypoQrGqKrPdxDV+M+tI2ROKVVGt3WR8wwTB0+0my1aNF3J8B34z68jiBWN86MSDGBsdQSRTB3/oxIPcsNuBsttNnOoxs46VOaFYFZXdbuIav5lZn8lqHymq3cSB38yszwz6QixmZtamgV6IxczMpmcgF2IxM7P+5MBvZlYxDvxmZhXjwG9mVjEO/GZmFeNePWZmfajMSdoc+M3M+kzZi9s41WNm1mfKnqTNgd/MrM94kjYzs4rxJG1mZhVzxP5z2treLgd+M7M+c+Od69ra3i4HfjOzPuMcv5lZxTjHb2ZWMV6IxcysYrwQi5lZBZW5EIsDv5lVSplz4AwKB34zq4yy58AZFG7cNbPKKHsOnEHhwG9mlVF2//hB4cBvZpVRdv/4QVF64Jc0JGmVpK+k93eTdL2kn6W/dy27DGZmUH7/+EHRjRr/u4A76u4vAW6IiOcBN6T3zcxKt3jBGB868SDGRkcQMDY6wodOPKhSDbtQcq8eSXsBxwAXAu9JN58AHJ7evgy4CTinzHKY2WRV7dZYZv/4QVF2d86Lgb8GdqrbtmdErAWIiLWS9mj2REmnA6cDzJs3r+RimlWLuzVWW2mpHknHAg9GxMrpPD8iLomIhRGxcM6cYuagNrOEuzVWW5k1/kXA8ZJeC2wH7CzpcuABSXPT2v5c4MESy2BmTbhbY7WVVuOPiPdGxF4RMR94I7A8It4MXAucmu52KvClsspgZs25W2O19aIf/1Lg1ZJ+Brw6vW9mXTSI3RqXrRpn0dLl7Lvkqyxaupxlq8Z7XaSB1ZW5eiLiJpLeO0TEr4FXduO8ZtZc2dP+Fs2N0cXyJG1mFdVpt8Zudgdt1RjtwN8+B34za1u3a+BujC6W5+oxs7Z1uzuoG6OL5cBvZm3rdg18EBuj+5lTPWYVN51c/bNGRxhvEuTLqoEPWmN0v3PgN5th2gnk083Vn330fls9D8qvgXuOneI41WM2g9QC+fiGCYKnA3lWn/fp5uo9y+Vgc43fbAZpt9tjJ7l618AHV+4av6QdyiyImXWu3UDu3jLVNGXgl3SYpJ+SLqYi6UWS/qX0kplZ29oN5O4tU015avwfAY4Gfg0QEbcCryizUGY2Pe0GcufqqylXjj8i7pVUv2lT1r5m1jvT6fboXH315An890o6DAhJ2wBnsPUaumbWRxzIbSp5Av/bgY8CY8B9wDeAd5RZKDOzqitzErwpA39EPAS8qZCzmZnZlMqeBC9Pr57LJI3W3d9V0qUdn9nMzJoqexK8PKmegyNiQ+1ORKyXtKCQs5tZYbo5P76Vq+xJ8PJ055wladfaHUm74RG/Zn2l3akarL+VPbAuT+D/B+C7ki6QdAHwXeD/FXJ2MytEt+fHt3KVPbAuT+PuZyStAI4EBJwYET8t5OxmVgivUDWzlD0NdWbgl7RzRDySpnZ+BXy+7rHdIuLhQkpgZltMN0/f7fnxrXxljsdoleqpBfqVwIq6n9p9MytQJ3l6z7lj7cis8UfEsUrmafiDiFjTxTKZVVK7UyrX8wpV1o6WOf6ICEnXAC/pUnnMKqvTPL2narC88nTLvEXSSyPih6WXxqzCepGnd9///tXTKRuAI4C3S7obeJykZ09ExMGFlMDMWLZqnCeefGrS9jLz9GVPC2DTV/bfJk/g/8OOz2JmmRr/yWtGR4Y5//gDSwvCnbQpWLnK/tu06s65B/A+4LnAbcCHIuKRjs9oZltp9k8OsMO2s6f8J+8kHdBOm4JTQt1V9riMVjX+z5B03fwn4FjgH4HTCjmrWQ/0a/Ca6p88q9ydpgPytik4JdR9Zbf3tOrH/8yIeH9EXBcRfwk4p28Dq5/nsmk1L0urcnc6TUPevv+eDqL7yh6X0SrwK52Cebd09O5Qw32zgdHPwavVP3mrchfR/TPPerueDqL7yl4LuVWqZxeSVE/9Yrs/Sn8H8OxCSmDWBf0cvFoNvjrzytVNn1Pbr9N0QJ6+/54OojfKHJfRauTu/FLOaNYD/R68sv7JW5X77KP3m9QbqIzun906j3VPnmmZzQbeoM5l06rcjemAXbcfZtvZszjzytUsWrq8sPaLstMO1n2KiF6XYUoLFy6MFSs8L5x1pl979UwlT7mbjQUYGR5ygK44SSsjYuGk7WUFfknbAd8CtiVJKf1nRJyXNgxfCcwH7gbeEBHrWx3Lgd+stUVLlzdNCY2NjnDzkiN7UCLrB1mBv9UArpY9d3LMx/874MiIeEzSMPAdSV8DTgRuiIilkpYAS4BzpnwFZgxurb1s/dx4bf2nVa+elSS9dwTMA9ant0eBNcC+rQ4cyVeJx9K7w+lPACcAh6fbLwNuwoHfcvBAomz93nht/SWzcTci9o2IZwPXAcdFxO4R8QySUbxX5zm4pCFJq4EHgesj4vvAnhGxNj3HWmCPjOeeLmmFpBXr1q1r60XZzNTPffF7bVAbr6038vTqeWlE/FftTkR8DfiDPAePiE0RcQiwF/AySS/MW7CIuCQiFkbEwjlz5uR9ms1gTmdkc88ba0ee2TkfknQucDlJqubNwK/bOUlEbJB0E/Aa4AFJcyNiraS5JN8GzKbkdEZrXojF8spT4z8FmANck/7MSbe1JGmOpNH09gjwKuBO4Frg1HS3U4EvtV1qqySnM8yKMWWNP+298y5JO0bEY1PtX2cucJmkIZIPmKsi4iuSvgdcJemtJI3EJ02n4FY9XlfWrBhT9uOXdBjwKWDHiJgn6UXAn0XEX3SjgOB+/NY5dwO1Ksrqx58n1fMR4GjSvH5E3Aq8otjimZWnn6dkNuuFXHP1RMS9DZsmLxdk1qfcDdRsa3l69dybpntC0jbAGcAd5RbLrDjuBmq2tTyB/+3AR4Ex4D7gG0DX8vtmnZqJ3UDdZmGdyJPq2S8i3hQRe0bEHhHxZuAFZRfMrCgzrRuo2yysU3kC/z/l3GbWl2baqFa3WVinWs3O+XLgMGCOpPfUPbQzMNT8WWa9lZUCmUmjWt1mYZ1qlePfBtgx3Wenuu2PAH9UZqHMpqPI2TuzPkDaya2XlYefiW0W1l15BnDtExH3dKk8TXkAl+VR1GIkWatZvf4lY3xx5XiuVa7KXBHLq21ZXp0M4PpUbc6d9EC7SrquyMKZTdeyVeMsWrqcfZd8tWnQh/ZTIFk59Cu+f2+u3PqyVeOcddWtpeXhZ1qbhXVfnu6cu0fEhtqdiFgvqekc+mbd1Kzm20y7KZCsD4pNGd+O6/evlSnPvp2YSW0W1n15avybJc2r3ZG0D8n0zGY91axm3mg63Tbb/aCo33+qMjkPb/0gT43//STr5X4zvf8K4PTyimSWz1S15yGJ17+k/Zrx2Ufvl+ubBEz+YGlVpuFZavoh5MFY1m15pmX+uqQXA79HsubumRHxUOklM8tQC5RTfe3cFMEXV46zcJ/dgKQ2Pr5hgiGJTRGMZQTZxumfW52nMbee1eMGSP57mrwWryNs3ZaZ6pG0f/r7xSSLrd8PjAPz0m1mXVc/ajWPiY2bOP/a27d6Ti3/3jjitb6h+KLr7uLso/fjl0uPYSwjPTM2OjIpODcbJVyzcVNMatz1YCzrhVY1/rOAtwH/0OSxAPL3jzObhvoUyC4jw0iw/omNbR9nw0T2c2ofDEBmzbtZ6ier7aD2QfDuK1c3PV8tFVR7bUX1RDJrR2bgj4i3pb+P6F5xzBKNKZBWwbtTGyY2cv61t2fWvGtjAPLm4RcvGMsM6s8aHcnVG8mNwFamVlM2nNjqiRFxdfHFMUvk6bFTpKwPllrNu93uk62+JUz12gZ5AjkbDK1SPcelv/cgmbNneXr/COAmwIHfStMvqY7p1rxbrQ98ZkYaCMhscDYrUqtUz58CSPoKcEBErE3vzwX+uTvFs6pq2TumBLtuP8xvN27OlcfPK+tbQtZra3dqCbPpyjOAa34t6KceAJ5fUnnMADhi/zkdPX9keIiR4VwrizIyPMR5xx3YtWkQZtr6ADZ48gzguimdm+cKkt48bwRuLLVUNmNMd3DSjXeua+s8oyPD7LDtbMY3TCBoq32gPsDXz8B55pWrt3TrLPIDoFUayKwb8gzgeqek15GM2AW4JCKuKbdYNhN0MjipnTSPeLpxdpZgc5sTitSXpVsDqjzXjvVSvu/C8CPgqxFxJnCdpJ2meoJZJ4OThtRkmGuGWpzfMLGx7aDfyAOqrAqmrPFLehvJ3Dy7Ac8hWXT948Aryy2aDbqsnjnjGyYmpYCO2H8ON965bsv9rNkti7br9sNb3ffqVlYFeWr87wAWkay8RUT8jKSLp1lLrbpCvueq1VstFn75LWu2up+/vj99Q7PEeccduNW2rDJ7QJXNJHkC/+8i4snaHUmz8bTMlkOrnjlTpWSKfoONDM9i+7pePrtuP8w/nPSiXHPtuMeNzTR5evV8U9L7gBFJrwb+AvhyucWyQZLVc6fdnjllGR0ZZvV5R+Xa1z1urAryrLkr4P8AR5F0oLgO+FRM9cQCec3d/rVs1Thn/+etbNy09dthdGS44/l1ptNDJ8vFJx/i4G2Vk7Xmbssav6RZwI8j4oXAJ8sqnA2uD3z59klBHzqfVG1keKjQuXo8x73Z01rm+CNiM3Br/dKLZvWmM01yM7OUfEsQSf5929l5exrnUz/9stkgqF8fYtHS5VvWjShCnv+uucDtkm6QdG3tp7ASWOUNSXz4DYew+ryj+MjJh/DbjZun/MYwS7DDNs0XPMmyYWJjof88ZmWpX3Co1tOtftGgTuUJ/B8AjgX+lmRRltqPGaMjw1PvNIVTDt17q0bVPCmenbcb5sLXHdS0B06rMnkglg2CsgcStlp6cTtJ7wZOAvYHbo6Ib9Z+Cjm7Dbzzjz+Q4Vmd9bq/4gf3bqnJ5B0o9ZuJjSxeMNZ0YrXzjz8w83keiGWDoOyBhK0ady8DNgLfBv4QOAB4VyFntYHSaqK12u+zrrp12qNtN20OPvDl21m8YCz3dMyzJJatGs+c8+YDX769afuDB2LZIMj6Pyjq/dsq1XNARLw5Ij4B/BHwvwo5o/WtZo1JeXONnU6xUAvSZx+9X65vEJsiWuY8zzvuQA/EsoFV9kDCVjX+LdWliHhKbUyaBSBpb+AzwDOBzSSzen5U0m7AlcB84G7gDRGxvr1iWxEaFzN//MmntnTNrAX47YZnNc01nnXVrZx55WpGtx/msd8+VWzBcr7VajnPZjV+D8SyQVb2+zdzAJekTcDjtbvACPBEejsiYueWB05W6pobET9KZ/NcCSwGTgMejoilkpYAu0bEOa2O5QFcxcuz4Hc3Cfjl0mNYtHR521My/3LpMaWVy2yQtT2AKyLa6ys3+flrgbXp7Ucl3UEys+cJwOHpbpeRrN/bMvBb8bq9mPlUatWPdhuvnLM3a1+xo2QySJoPLAC+D+xZW8ox/d10pk9Jp0taIWnFunX9MefLTJI3wI6ODE/KNZZhLA3g7QRy5+zNpqf0wC9pR+CLwLsj4pG8z4uISyJiYUQsnDOns/VXbbI8AXZkeIhjXzSXbkzGWgvgzRq1aoaHtGV0b5lr4prNdHlm55w2ScMkQf9zEXF1uvkBSXMjYm3aDvBgmWWw5s4+er9JOf7hWWLH7Waz4YmNWxZH+eLKcSY2bi61LKMjw5O6h1503V2Mb5hgSGJTBGNunDUrTGmBP53V89+AOyLiw3UPXQucCixNf3+prDJUQZ7FzFvt0+q5i5YuL70dYGR4aNKAK69Ha1auKadlnvaBpd8nGfx1G0l3ToD3keT5rwLmAWuAkyLi4VbHcq+e5pr1zBkZHtoqBdJsn8aafVZNet8lX22Z5BH5k0DN9h0dGeb84w90kDcrybSmZe5ERHyH7B7ZXq+3AK3m82g1983GzbFlwFStvz5MnrK41SjakeEhXjxvF773Pw/nmjO/2S47bDvbQd+sB7rSq8fKkWc+jzy9d7Imf8pqaN11+2Fe/5IxfrTmN5OC/ujI8KQFzLMUOW9OmVPYmvVCr6dltj6V1TOnNo9Nq30aNQvCzSZBu/jkQ1j1N0dx453rmub/d9g2SSF1Uv52lT2FrVm3lf2eLrVXj5Xr7KP34+z/uJWNDdXu2jw2tX3yjNDNCsJZDa2tvm3kmWityD74eVJeZoOk7Pe0a/yDLqMVpf5NUqu1AzSbcml4lnjiyafa+kqZ9UFRayxuTBGV2Qe/7Clszbqtl9MyWx9p1iXzouvuarrebU3tTVILsM1q/iPDs3gqZ2NvvWbfJGq1+CImmMrTTbWm7Clszbqt7Pe0A/8AaOySWQvO7aRvsubmefKpmDSlcp6vlM0GWtU3EnfSFz/r9daft16rDyGzQVT2e9qBfwBk5ftaaXyTZH1FzJpH//4NE1PWupt9k8j7jWGqQWXt5Dc9BbPNNGW/px34B0C7eb1dtx/mvOO2HhiV9dWxNiVCo11GhnMF9Ok0Qk1Vo59OftOjfW2mKfM97cbdAZA3rzckbelu2fiGyVrR55RD9266XZr8raJZf//pBOmpFpJu1XBsZp1z4O8DUw3UaDVjZb3NEU1rCLW0ysTGTQyl3XpqPWs+uPigpguWZ/XFbwzoWcF4l5HsQVxZXT1rxy572TmzqnOqp8fyNGQ25vtmZaRnmgXhxuNvipjU+6bZV8pao+1U58gaS7BhYiPnLruNDy4+aFJ5sub4qR3bOXuzcrnG32NTpT0aBUnNvtHwkCbViJetGuesq25t6/g1eWvdixeMseN2zesPn7tlzaRvLxddd1fToC/Y6tiLF4xx85Ij+eXSY7h5yZEO+mYFco2/x7Jy4eMbJpi/5KtbGl/ra8lN++E0bKzV9Fv12mmlnVp3Vloo0ufXPyfrvEHrXkDNtNPX38ye5sDfY1NNb1AL3FNNgLlxc0w5K2fjeaeSt1dBq9fQrE2g2b5jbTbcttvX38ye5lRPj+VtuM0j76ycRTeUtjrWaMNMnUU13LabIjOzpznw91jjDJidmCVt6RnUGHDr1QJkUTP9LV4wxshw87dSY6ap2Yyf05m3x/PzmE2fUz19oD6lsmjp8ilntsxSSwuNb5hgeJYYHlLmXD5Fp0Z+m7Eu728mJuf/ixiY4vl5zKbPNf4curnIR6vUT+0bQa0vfuPvehs3BztsM3tLzbrZPkWmRro96Mp9/c2mzzX+KXS7EbHZ5GebIhibYm3cZn4zsZHV5x3Vcp+iUiPdnijNff3Nps+Bfwq9WOSj3VRInrRH2amRXgRiz89jNj0O/FMYhEbEPLXtbtTIHYjNBoMD/xQGoRExT23bqREzq1FkjOzsJwsXLowVK1b05NyNOX5IaspFLh1oZlYGSSsjYmHjdtf4p1B2TdnTDphZtznw51BW7rrTHkP+0DCz6XA//h7qZNqB2ofG+IYJgqc/NMocY2BmM4Nr/D00nR5DtVp+swbnrG6m/mZgZvUc+Huo3R5DzRqaGzV+aHgWSzNr5FQP3Z2SoV670w5MNdUyTP7Q8CyWZtao8jX+XtaI2+0xNNWgsWYfGlOlk5wGMqueygf+vFMyTBUgz112G1d8/142RTAkccqhe09ab7aZdnoMtVrwJGsun1bpJKeBzKqp8qmePA2sU/WgedMnv8flt6zZMi3ypgguv2UN5y67rdCyZqWGLj75kMx1aVulk5wGMqumygf+PNMJtwqQy1aNc/MvHm56jCu+f29xBWV6i5i0es4gzENkZsWrfKonz+RlrQJkq9px1kLnnZjOYLKs5wzCPERmVrzK1/jz1KJbfStoVTtutvhJP/FiJmbVVFqNX9KlwLHAgxHxwnTbbsCVwHzgbuANEbG+rDI0ymqgnaoW3epbQdZgKoBTDt27rXJ0m2fsNKumMlM9nwY+BnymbtsS4IaIWCppSXr/nBLLsEUnPVhW3PMwv3uqPujP2upbQbNBVYues1vTXj391pPGc+ibVU9pqZ6I+BbQ2Op5AnBZevsyYHFZ52803R4s5y67jctvWcPmunT9xMbNrLgneWnNUkUXn3wIn3vbywsth5lZUbrduLtnRKwFiIi1kvbI2lHS6cDpAPPmzev4xNPtwZLVM+eK79+7pUbfTq25qJ40/ZIuMrPB07eNuxFxSUQsjIiFc+bM6fh4ebptNpPVM2e6PXamW456npnTzDrR7cD/gKS5AOnvB7t14un2YMnqmTPdHjtF9KRxusjMOtHtwH8tcGp6+1TgS9068XQGP0F2z5ys7WWVo54HXplZJ8rsznkFcDiwu6T7gPOApcBVkt4KrAFOKuv8zUynB0stjz+deXiKLEc9D7wys054sfUB5AXgzSwPL7Y+g3jglZl1YsYG/pne3dEDr8xsumZk4C9jdOxM/yAxs+qYkYE/7+IqefXbNAu1MvmDyMymo28HcHWi6O6O/dZv3gO4zKwTMzLwFzE6tl6/9Zvvtw8iMxssMzLwFz3PfNEfJJ3qtw8iMxssMzLwtzM6dtmqcRYtXc6+S77KoqXLm6ZL+m3Bkn77IDKzwTIjG3chX3fHvI22/dZvPs9ykWZmWWZs4M+jnd4//dRvvt8+iMxssFQ68A9yrryfPojMbLDMyBx/Xs6Vm1kVVTrw91ujrZlZN1Q61eNcuZlVUaUDPzhXbmbVU+lUj5lZFTnwm5lVjAO/mVnFOPCbmVWMA7+ZWcUMxGLrktYB9/S6HG3aHXio14XoM74mW/P1mMzXZLJOrsk+ETGnceNABP5BJGlFs9Xtq8zXZGu+HpP5mkxWxjVxqsfMrGIc+M3MKsaBvzyX9LoAfcjXZGu+HpP5mkxW+DVxjt/MrGJc4zczqxgHfjOzinHgL4CkSyU9KOknddt2k3S9pJ+lv3ftZRm7SdLekm6UdIek2yW9K91e5WuynaQfSLo1vSYfSLdX9poASBqStErSV9L7Vb8ed0u6TdJqSSvSbYVfEwf+YnwaeE3DtiXADRHxPOCG9H5VPAWcFREvAH4PeIekA6j2NfkdcGREvAg4BHiNpN+j2tcE4F3AHXX3q349AI6IiEPq+u4Xfk0c+AsQEd8CHm7YfAJwWXr7MmBxN8vUSxGxNiJ+lN5+lOQfe4xqX5OIiMfSu8PpT1DhayJpL+AY4FN1myt7PVoo/Jo48Jdnz4hYC0kgBPbocXl6QtJ8YAHwfSp+TdK0xmrgQeD6iKj6NbkY+Gtgc922Kl8PSCoD35C0UtLp6bbCr0nlV+Cy8kjaEfgi8O6IeERSr4vUUxGxCThE0ihwjaQX9rhIPSPpWODBiFgp6fAeF6efLIqI+yXtAVwv6c4yTuIaf3kekDQXIP39YI/L01WShkmC/uci4up0c6WvSU1EbABuImkXquo1WQQcL+lu4AvAkZIup7rXA4CIuD/9/SBwDfAySrgmDvzluRY4Nb19KvClHpalq5RU7f8NuCMiPlz3UJWvyZy0po+kEeBVwJ1U9JpExHsjYq+ImA+8EVgeEW+motcDQNIOknaq3QaOAn5CCdfEI3cLIOkK4HCS6VMfAM4DlgFXAfOANcBJEdHYADwjSfp94NvAbTydv30fSZ6/qtfkYJKGuSGSCtdVEfG3kp5BRa9JTZrq+auIOLbK10PSs0lq+ZCk4T8fEReWcU0c+M3MKsapHjOzinHgNzOrGAd+M7OKceA3M6sYB34zs4px4LeBJykkfbbu/mxJ62ozPrZxnJskLUxv/1et332HZTstLctqST+V9LaM/Y6XVMUJyawHPGWDzQSPAy+UNBIRE8CrgfFODhgRry2kZIkrI+Kd6TD82yVdGxEP1B6UNDsiriUZqGNWOtf4bab4GslMjwCnAFfUHkhHRF4q6Yfp3O8npNtHJH1B0o8lXQmM1D3nbkm7p7eXpZNm3V43cRaSHpN0YTrH/i2S9mxVwHQY/i+AfSR9WtKHJd0I/F36zeBj6XH3lHRNetxbJR2Wbn9zOqf/akmfkDRUwHWzCnLgt5niC8AbJW0HHEwySrjm/SRTArwUOAK4KB0S/+fAExFxMHAh8JKMY78lIl4CLATOSEdSAuwA3JLOsf8toGkapyYdmfls4OfppucDr4qIsxp2/Ufgm+lxX0zyLeEFwMkkk3gdAmwC3tTqfGZZnOqxGSEifpxOAX0K8F8NDx9FMiHYX6X3tyMZ/v4KkiBbe/6PMw5/hqTXpbf3Bp4H/Bp4Eqi1I6wkSTE1c3I6jcXvgD+LiIfTmUr/I52xs9GRwJ+k5doE/EbSH5N8MP0wfe4IFZvAzIrjwG8zybXA35PMm/SMuu0CXh8Rd9XvnAbQlnOWpPPIvAp4eUQ8Iekmkg8OgI3x9Jwnm8j+f7oyIt7ZZPvjrc7dWBTgsoh4bxvPMWvKqR6bSS4F/jYibmvYfh3wl+msoUhakG7/Fmm6JJ0b/+Amx9wFWJ8G/f1JlpIs2w0kaaja4i07p9v+KG0grq3Duk8XymIzkAO/zRgRcV9EfLTJQxeQLHX4Y0k/Se8D/CuwY5ri+WvgB02e+3VgdrrPBcAtxZd8kncBR0i6jSSFdGBE/BQ4l2R1ph8D1wNzu1AWm4E8O6eZWcW4xm9mVjEO/GZmFePAb2ZWMQ78ZmYV48BvZlYxDvxmZhXjwG9mVjH/H8uHPoEQoybVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(Y_test, Y_pred)\n",
    "plt.xlabel(\"Median Price\")\n",
    "plt.ylabel(\"Predicted Price\")\n",
    "plt.title(\"Median Price vs Predicted Price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the endpoint\n",
    "\n",
    "Since we are no longer using the deployed model we need to make sure to shut it down. Remember that you have to pay for the length of time that your endpoint is deployed so the longer it is left running, the more it costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Clean up\n",
    "\n",
    "The default notebook instance on SageMaker doesn't have a lot of excess disk space available. As you continue to complete and execute notebooks you will eventually fill up this disk space, leading to errors which can be difficult to diagnose. Once you are completely finished using a notebook it is a good idea to remove the files that you created along the way. Of course, you can do this from the terminal or from the notebook hub if you would like. The cell below contains some commands to clean up the created files from within the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will remove all of the files contained in the data_dir directory\n",
    "!rm $data_dir/*\n",
    "\n",
    "# And then we delete the directory itself\n",
    "!rmdir $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
