{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#14012022\n",
    "\n",
    "Concepts reviewed:\n",
    "\n",
    "Training and Testing Sets = Training model and use Testing to verify how good model performs. You never use testing data to train the model. \n",
    "Overfitting and Underfitting => Underfitting means the model is too simplistic and tend to oversimplifed the problem. Called also error due to bias. \n",
    "Overfittin is the opposite. Model is way too specific and performs pefectly on training set but on testing it wil fail as it cannot generalize that well. Called also error due to variance. \n",
    "Early Stopping => We do gradient descent until the testing error stops and starts increasing. That is the right spot and called early stopping. \n",
    "Regularization (L1 and L2) => T heproblem is that large coefficients are leading to it.We want to penalize in our error function large weights by either having a sum of the absolute values of the weights multiplied by lambda or the sum of the squares mulitplied by lambda. \n",
    "The first approach is called L1 Regularization. L1 tends to result in sparse vectors means that small weights tend to go to zero. Also this is good if we want to reduce the number of weights and have a small set. For feature selection is good because we can choose the most important ones and turn the rest to zero.\n",
    "The second approach is called L2 Regularization and it is good for training purposes. \n",
    "L2 does not favor sparse vectors since it tries to mantain all the weights homogeneously small. \n",
    "\n",
    "Dropout => a parameter given when training. e.g. 0.2 means 20% of the nodes will be turned off at each epoch. Some nodes may be never turned off and some more than others but in average each node will be turned off equally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#15012022\n",
    "\n",
    "Local Minima => When there are multiple low points and gradient descent may end up not at the lowest point but the lowest local point.\n",
    "To avoid this we can do a random restart from different positions and do gradient descent from all of them. \n",
    "Momentum => Another way to avoid local minima. The general idea is that the step to take will be an average of previous steps. Previous step multiplied by 1, previous one by Beta, previous one By Beta squared etc.. \n",
    "Vanish gradient problem => The sigmoid function gets pretty flat on the sides, so when calculating the derivatives at a point fat to right or left, the derivative is almost zero. \n",
    "This is a problem because the derivative is what tells you where to move towards and it would be very tiny steps.\n",
    "\n",
    "Hyperbolic Tangent Function => Another activation function. e^x - e^-x/e^x+ e^x.\n",
    "The values returned are between -1 and 1. \n",
    "\n",
    "ReLU => Rectified Linear Unit. Simple activation function. It return x if x is positive, if x is negative returns 0. ReLU is often used instead of Sigmoid. \n",
    "\n",
    "Stochastic gradient descent => If the data is well-distributed we can subset the data and run each subset throught the network, calculate the error and backpropagate. Then you repeat for next subset. \n",
    "Each step (epoch) may be less accurate than running once with all the data but in practice it is better to take a bunch of slightly inaccurate steps than take only one good one. \n",
    "\n",
    "Learning Rate => If learning rate is too big, you take too big steps. Too low learning rate will lead to too slow model and training. \n",
    "As a rule of thumb if the model is not working, you should decrease the learning rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#19112022\n",
    "\n",
    "Better initialization of the weights => In Sentiment Analysis prob 3, weights for the hidden layer to output were initialized as ```self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5, (self.hidden_nodes, self.output_nodes))```\n",
    "\n",
    "However, there is a better way of doing it. Normally the weights when initialized should be set to be close to zero without being too small.\n",
    "Good practice is to start the weights in the range of [-y, y] where y is 1/‚àön where n is the number of inputs of the layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#25012022\n",
    "\n",
    "`Softmax =>` The softmax function is used to calculate probabilities. This function can be used in a classifier only when the classes are mutually exclusive. \n",
    "The values returned will sum up to 1. \n",
    "<img src=\"pics/softmax_function.png\">\n",
    "\n",
    "<img src=\"pics/softmax_symbols.png\">\n",
    "\n",
    "\n",
    "The term on the bottom of the formula is the normalization term. It ensures that all the output values of the function will sum to 1 and each be in the range (0, 1), thus constituting a valid probability distribution.\n",
    "\n",
    "Simply put it you do the exponent (e^n) for each element in the vector, the sum of the value is the denominator. Then you divide each element of the input vector by the denominator and that will give you the probability. All of them summed up will result to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#26012022\n",
    "\n",
    "Loss function is used to be able to adjust the weight when backpropagation is done. The loss function calculates how bad the predictions were. A typical loss function is the mean squared loss.\n",
    "\n",
    "$$\n",
    "\\large \\ell = \\frac{1}{2n}\\sum_i^n{\\left(y_i - \\hat{y}_i\\right)^2}\n",
    "$$\n",
    "\n",
    "where $n$ is the number of training examples, $y_i$ are the true labels, and $\\hat{y}_i$ are the predicted labels.\n",
    "\n",
    "\n",
    "For a classification problem such the MNIST were a softmax is used to get the probabilities, the CrossEntropyLoss is the function to be used for calculating the loss. \n",
    "However with the CrossEntropyLoss in Pytorch you need to feed the raw output and not the output of the softmax function.\n",
    "\n",
    "Alternatively you can build the model with nn.LogSoftMax or F.log_softmax. Probabilities can be then obtained by doing the exponential (torch.exp(output)). With log_softmax you would need to use the NLLLoss (Negative likelihood loss).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#29012022\n",
    "\n",
    "Inherence is the process of making predictions.\n",
    "\n",
    "Pytorch notes:\n",
    "\n",
    "model.eval() is used to turn off dropout when doing validation\n",
    "Once one epoch is done, turn back on the dropout by using model.trai(). Always remember to do the whole validation with no_grad. \n",
    "\n",
    "To save and load a model in Pytorch it is important to know that when loading it back to another model, it will need to have the same architecture of the one saved.\n",
    "When saving the model, you can create a checkpoint dictionary:\n",
    "\n",
    "```\n",
    "checkpoint = {'input_size': 784,\n",
    "              'output_size': 10,\n",
    "              'hidden_layers': [each.out_features for each in model.hidden_layers],\n",
    "              'state_dict': model.state_dict()}\n",
    "```\n",
    "then save the model\n",
    "torch.save(checkpoint, 'checkpoint.pth')\n",
    "\n",
    "To load it back you can create a custom function to do that. Example:\n",
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath) => Load the pth file\n",
    "    model = fc_model.Network(checkpoint['input_size'], => Create the model based on the checkpoint content\n",
    "                             checkpoint['output_size'],\n",
    "                             checkpoint['hidden_layers'])\n",
    "    model.load_state_dict(checkpoint['state_dict']) => Load state dict\n",
    "    \n",
    "    return model\n",
    "    \n",
    "\n",
    "Then you can just simply do:\n",
    "\n",
    "model = load_state_dict('checkpoint.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#30012022\n",
    "\n",
    "Convolutional Neural Networks CNN\n",
    "\n",
    "Data Normalization is a pre-processing step which ensure that each input (in case of images), so each pixel is coming with a standard distribution. For example in MNIST hand-written numbers dataset the values of the pixelf would be from 0 to 255 (Black and white pics). However since the net workds with gradients we normalize the data to be between 0 and 1.\n",
    "\n",
    "Data normalization is typically done by substracting the mean of the pixel values from each pixel and then dividing the result by the standard deviation of all the pixel values. \n",
    "\n",
    "The distribution of this data will resemble of a Gaussian function. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#31012022\n",
    "\n",
    "CrossEntropyLoss in Pytorch has two steps, softmax and negative log likelihood loss.\n",
    "If using CrossEntropyLoss as criterion, then you do not need to specify the softmax as output layer.\n",
    "However, it is possible to alternatively define a log softmax layer in the model architecture and then use the NLLLoss as criterion. \n",
    "\n",
    "When building a model, we should always use 3 dataset, training, validation and testing.\n",
    "The validation is used while training but it is used to verify how well the model can generalize. With the validation set we do not update weights.\n",
    "The number of epochs to be used should be when the validation loss starts increasing and training loss keeps decreasing. That is when the model starts overfitting. \n",
    "The data for test is data has never been seen before by tbe model and should be used to verify how accurate the model is. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#01022022\n",
    "\n",
    "Differences between MLPs and CNNs\n",
    "\n",
    "MLPs use only fully connected layers while CNNs also use sparsely connected layers. In MLPs there is a lot of parameters and even with relatively small picture this number is high, resulting in higher computational needs. \n",
    "\n",
    "MLPs only accepts vectors (flattening the image) while CNNs also accept matrices. The fact of flattening an image can lose some information about the position of the pixels within the image.\n",
    "\n",
    "\n",
    "Convolutional Layer = Convolutional Kernel\n",
    "\n",
    "Spatial patterns in an image are related to either color or shape.\n",
    "\n",
    "Filters: to detect changes in intensity in an image, we use filters that look at groups of pixels and react to alternative patterns of dark/light pixels.These filters produce an output that shows edges of an object and differing texture. \n",
    "\n",
    "In images there is a rate of change. Similarly to audio there is high frequency and low frequency. \n",
    "\n",
    "Most images have both high-frequency and low-frequency components. high-frequency image pattern; this part changes very rapidly from one brightness to another. Instead where there is a  change very gradually, which is considered a smooth, low-frequency pattern.\n",
    "\n",
    "<img src='pics/high_low.png'>\n",
    "\n",
    "\n",
    "High pass  filters are applied to images to detect edges. Edges are areas in image where the intensity changes very quickly and very often indicates object boundaries. \n",
    "\n",
    "<img src='pics/Edge_detection_filter.png'>\n",
    "The sum of the matric has to be zero. If the result is positive the image will get brighter, if negative darker. Note that the values in orange summed are the value in the middle. \n",
    "<img src='pics/Convolutional_kernel.png'>\n",
    "\n",
    "Convolution is represented by asteriks, not multiplication\n",
    "\n",
    "<img src='pics/Convolutional_kernel_applied.png'>\n",
    "\n",
    "To find and enhancing an horizontal edge and line you would want to use for example the below kernel:\n",
    "\n",
    "-1 -2 -1\n",
    " 0  0  0\n",
    " 1  2  1\n",
    "\n",
    " This way you can see the difference between bottom and top\n",
    "\n",
    " Sobel filter is commonly used for edge detection and finding patterns in an image. Using the Sobel filter to an image is a way to taking an approximation of the derivative of the image in the x or y direction. \n",
    "\n",
    "\n",
    "Sx = [[-1 0 1],\n",
    "      [-2 0 2],\n",
    "      [-1 0 1]]\n",
    "\n",
    "Sy = [[-1  -2  -1],\n",
    "      [ 0   0   0],\n",
    "      [ 1   2   1]]\n",
    "\n",
    "\n",
    "\n",
    "A basic CNNs has the below structure:\n",
    "\n",
    "<img src='pics/CNN.png'>\n",
    "\n",
    "An image is passed through a filter, in this case 4 convolutional kernels which produce four differently filtered images which are then stacked together. This is the convolutional layer. \n",
    "\n",
    "<img src='pics/CNN2.png'>\n",
    "<img src='pics/CNN3.png'>\n",
    "\n",
    "Grey scale picture are 2D array, height and width.\n",
    "Colored pictures (RGB) are 3D array, height, width and depth. For RGB the depth is 3. \n",
    "A stack of 3 2D matrices. \n",
    "\n",
    "\n",
    "If you take a picture and apply 4 filters, both for detecting horizontal and vertical edges, there will be 4 collections of nodes, called also as feature maps or activation maps:\n",
    "\n",
    "<img src='pics/FeatureMaps.png'>\n",
    "\n",
    "<img src='pics/FeatureMaps2.png'>\n",
    "\n",
    "Note that in the picture above you can see the first two filters are discovering vertical edges. The first one has lighter features on the right, the second on the left. \n",
    "\n",
    "Edges and images appear as a line of lighter pixel next to darker pixels. \n",
    "\n",
    "Also on colored picture you apply filters which in this case are 3D as well, a stack of 3 2D matrices.\n",
    "\n",
    "<img src='pics/RGB_Convolutional.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#02022022\n",
    "\n",
    "Stride is how many pixel the filters will move when applied. For example if strid is 1, it will move 1 pixel at a time. \n",
    "When applying bigger strides you may end up having the filter going over the boundary of the image. In this case you could either ignore those regions and losing part of the information from the image or using padding so , adding 0s to the regions that were not covered by the filter. \n",
    "\n",
    "FILTER APPLIED (2x2):\n",
    "<img src='pics/Padding.png'>\n",
    "\n",
    "Ignore the regions not covered by the filter:\n",
    "\n",
    "<img src='pics/Ignore.png'>\n",
    "\n",
    "Or you could apply a padding by adding zeroes so that you retain all the information:\n",
    "\n",
    "<img src='pics/Padding2.png'>\n",
    "\n",
    "\n",
    "Pooling Layers:\n",
    "\n",
    "Pooling is the layer that takes in aa input a convolutional layer and returns as output the same number of feature maps but reduced. \n",
    "There are different pooling, e.g. max pooling and avg pooling. \n",
    "\n",
    "In Max pooling, once assigned a window size and a stride, the pooling will select the max number within the window:\n",
    "\n",
    "1 9 6 4\n",
    "5 4 7 8\n",
    "5 1 2 9\n",
    "6 7 6 0\n",
    "\n",
    "You start from left top and you would have\n",
    "1 9\n",
    "5 4\n",
    "\n",
    "In this case it takes 9\n",
    "\n",
    "Stride is 2 so next will be\n",
    "6 5\n",
    "7 8\n",
    "\n",
    "In this case take 8\n",
    "\n",
    "Repeat this and you get\n",
    "\n",
    "9 8\n",
    "7 9\n",
    "\n",
    "So from a 4x4 it is now a 2x2. \n",
    "\n",
    "The average pooling instead of return an avg of the window size instead of the max. However in image classification, max pooling is more effective and notices better edges, without missing details. \n",
    "\n",
    "<img src='pics/MaxPooling.png'>\n",
    "\n",
    "<img src='pics/MaxPooling2.png'>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#03022022\n",
    "\n",
    "Remember that pooling throws away some information of an image which may cause some issues, for example in Face recognition.\n",
    "An alternative to pooling is Capsule Networks. Capsule Networks provide a way to detect parts of objects and represent spatial relationships between them. Capsule networks are made of parent and child nodes that build up a complete picture of an object.\n",
    "\n",
    "<img src=\"pics/Capsule_Net.png\">\n",
    "\n",
    "Capsule are basically a collection of nodes, each one containing information about a specific part of the object, e.g. width, orientation, color etc.\n",
    "\n",
    "Each capsule outputs a vector with some magnitude and orientation:\n",
    "\n",
    "Magnitude (m) is the probability that a part exists, a value between 0 and 1.\n",
    "Orientation (Œ∏, theta) is the state of the part properties. \n",
    "To check this repo: https://github.com/cezannec/capsule_net_pytorch\n",
    "\n",
    "\n",
    "Depth: Colored pics have 3, Black and white 1. \n",
    "When working with images, we need to resize them before doing anything. The resizing typically means means to resize the pics as a square. \n",
    "All pics have to have a fixed size also for CNNs with spatial dimension equal to a power of 2 or else a number that is divisible by a large power of 2.\n",
    "\n",
    "How to define a conv layer in Pytorch, example:\n",
    "\n",
    "self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "First 3 is the depth of the image, so if a color image it will be 3\n",
    "16 is for how many filters we want to have\n",
    "The second 3 is the kernel_size, so in this case a 3x3. Typically it can be from 2x2 to 7x7 or up for bigger pics.\n",
    "padding=1 is to make sure we do not loose information from the borders of the pics\n",
    "\n",
    "<img src=\"pics/Conv_Layers.png\">\n",
    "\n",
    "Note that for each conv layer, the number of filters is doubling, and so is the depth. Depth is the filter number of the prev layer.\n",
    "\n",
    "Next is the addition of Pooling so that the dimensions of x, y get cut in half in the case of using a kernel and stride of 2 and 2:\n",
    "\n",
    "<img src=\"pics/PyTorch_MaxPooling.png\">\n",
    "\n",
    "QUESTIONS: \n",
    "\n",
    "How might you define a Maxpooling layer, such that it down-samples an input by a factor of 4? \n",
    "nn.MaxPool2d(4,4) or nn.MaxPool2d(2,4). \n",
    "Since the stride is 4 you would downsample by 4 but it is best to go for 4,4 so that the maxpooling function sees every input pixel once.\n",
    "\n",
    "If you want to define a convolutional layer that is the same x-y size as an input array, what padding should you have for a kernel_size of 7? (You may assume that other parameters are left as their default values.)\n",
    "\n",
    "padding=3\n",
    "\n",
    "E.g. if your pic is 32x32 and kernel is 7x7, after 4 iterations, we are at pixel 28, the kernel will overlay 3 columns that do not exist. \n",
    "\n",
    "nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)\n",
    "in_channels refers to the depth of an input. For a grayscale image, this depth = 1\n",
    "out_channels refers to the desired depth of the output, or the number of filtered images you want to get as output\n",
    "kernel_size is the size of your convolutional kernel (most commonly 3 for a 3x3 kernel)\n",
    "stride and padding have default values, but should be set depending on how large you want your output to be in the spatial dimensions x, y\n",
    "\n",
    "\n",
    "The number of parameters is the x-y size of the final output times the number of final channels/depth, so if pic is 16*16 and last conv layer was filter 20, then it is 16*16*20\n",
    "\n",
    "<img src=\"pics/CNN_Prediction.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#05022022\n",
    "\n",
    "To compute the output size of a convolutional layer we can obtain it from the below formula\n",
    "\n",
    "(W‚àíF+2P)/S+1\n",
    "\n",
    "So if you had a 7x7 input with a 3x3 kernel, padding 0 and stride of 1, the output would be a 5x5: ((7-3+0)/1)+1=5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#06022022\n",
    "\n",
    "When initializing weights for a model it makes sense to have them based on the amount of nodes we have in input. \n",
    "The more inputs a certain node sees the smaller the weight should be.\n",
    "\n",
    "Good practice is to start your weights in the range of  [‚àíùë¶,ùë¶]  where  ùë¶=1/sqr.root(n)  is the number of inputs to a given neuron)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#07022022\n",
    "\n",
    "Autoencoder: you can think the process of an image going through convolutional layers as a sort of decompression. Autoencoders compress and decompress data. \n",
    "\n",
    "<img src='pics/Autoencoders.png'>\n",
    "\n",
    "An autoencoder has two parts. Encoder that compresses the data and a decoder that reconstruct the data from the compressed one. This is also used in for example GANs.\n",
    "They work well for de-noising and also filling in missing data. \n",
    "\n",
    "<img src='pics/DenoiseImageTrans.png'>\n",
    "\n",
    "The middle layer is the compressed representation of the input from which you can reconstruct the data. Key aspect of an autoencoders is that it can compresse the data such that the content is still maintained. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9244e6e8c562ea0ec0726a39210e7e093201dbedd1fa49bbd40fbf29c80fe905"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
