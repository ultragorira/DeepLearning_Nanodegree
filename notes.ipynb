{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#14012022\n",
    "\n",
    "Concepts reviewed:\n",
    "\n",
    "Training and Testing Sets = Training model and use Testing to verify how good model performs. You never use testing data to train the model. \n",
    "Overfitting and Underfitting => Underfitting means the model is too simplistic and tend to oversimplifed the problem. Called also error due to bias. \n",
    "Overfittin is the opposite. Model is way too specific and performs pefectly on training set but on testing it wil fail as it cannot generalize that well. Called also error due to variance. \n",
    "Early Stopping => We do gradient descent until the testing error stops and starts increasing. That is the right spot and called early stopping. \n",
    "Regularization (L1 and L2) => The problem is that large coefficients are leading to it.We want to penalize in our error function large weights by either having a sum of the absolute values of the weights multiplied by lambda or the sum of the squares mulitplied by lambda. \n",
    "The first approach is called L1 Regularization. L1 tends to result in sparse vectors means that small weights tend to go to zero. Also this is good if we want to reduce the number of weights and have a small set. For feature selection is good because we can choose the most important ones and turn the rest to zero.\n",
    "The second approach is called L2 Regularization and it is good for training purposes. \n",
    "L2 does not favor sparse vectors since it tries to mantain all the weights homogeneously small. \n",
    "\n",
    "Dropout => a parameter given when training. e.g. 0.2 means 20% of the nodes will be turned off at each epoch. Some nodes may be never turned off and some more than others but in average each node will be turned off equally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#15012022\n",
    "\n",
    "Local Minima => When there are multiple low points and gradient descent may end up not at the lowest point but the lowest local point.\n",
    "To avoid this we can do a random restart from different positions and do gradient descent from all of them. \n",
    "Momentum => Another way to avoid local minima. The general idea is that the step to take will be an average of previous steps. Previous step multiplied by 1, previous one by Beta, previous one By Beta squared etc.. \n",
    "Vanish gradient problem => The sigmoid function gets pretty flat on the sides, so when calculating the derivatives at a point fat to right or left, the derivative is almost zero. \n",
    "This is a problem because the derivative is what tells you where to move towards and it would be very tiny steps.\n",
    "\n",
    "Hyperbolic Tangent Function => Another activation function. e^x - e^-x/e^x+ e^x.\n",
    "The values returned are between -1 and 1. \n",
    "\n",
    "ReLU => Rectified Linear Unit. Simple activation function. It return x if x is positive, if x is negative returns 0. ReLU is often used instead of Sigmoid. \n",
    "\n",
    "Stochastic gradient descent => If the data is well-distributed we can subset the data and run each subset throught the network, calculate the error and backpropagate. Then you repeat for next subset. \n",
    "Each step (epoch) may be less accurate than running once with all the data but in practice it is better to take a bunch of slightly inaccurate steps than take only one good one. \n",
    "\n",
    "Learning Rate => If learning rate is too big, you take too big steps. Too low learning rate will lead to too slow model and training. \n",
    "As a rule of thumb if the model is not working, you should decrease the learning rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#19112022\n",
    "\n",
    "Better initialization of the weights => In Sentiment Analysis prob 3, weights for the hidden layer to output were initialized as ```self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5, (self.hidden_nodes, self.output_nodes))```\n",
    "\n",
    "However, there is a better way of doing it. Normally the weights when initialized should be set to be close to zero without being too small.\n",
    "Good practice is to start the weights in the range of [-y, y] where y is 1/√n where n is the number of inputs of the layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#25012022\n",
    "\n",
    "`Softmax =>` The softmax function is used to calculate probabilities. This function can be used in a classifier only when the classes are mutually exclusive. \n",
    "The values returned will sum up to 1. \n",
    "<img src=\"pics/softmax_function.png\">\n",
    "\n",
    "<img src=\"pics/softmax_symbols.png\">\n",
    "\n",
    "\n",
    "The term on the bottom of the formula is the normalization term. It ensures that all the output values of the function will sum to 1 and each be in the range (0, 1), thus constituting a valid probability distribution.\n",
    "\n",
    "Simply put it you do the exponent (e^n) for each element in the vector, the sum of the value is the denominator. Then you divide each element of the input vector by the denominator and that will give you the probability. All of them summed up will result to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#26012022\n",
    "\n",
    "Loss function is used to be able to adjust the weight when backpropagation is done. The loss function calculates how bad the predictions were. A typical loss function is the mean squared loss.\n",
    "\n",
    "$$\n",
    "\\large \\ell = \\frac{1}{2n}\\sum_i^n{\\left(y_i - \\hat{y}_i\\right)^2}\n",
    "$$\n",
    "\n",
    "where $n$ is the number of training examples, $y_i$ are the true labels, and $\\hat{y}_i$ are the predicted labels.\n",
    "\n",
    "\n",
    "For a classification problem such the MNIST were a softmax is used to get the probabilities, the CrossEntropyLoss is the function to be used for calculating the loss. \n",
    "However with the CrossEntropyLoss in Pytorch you need to feed the raw output and not the output of the softmax function.\n",
    "\n",
    "Alternatively you can build the model with nn.LogSoftMax or F.log_softmax. Probabilities can be then obtained by doing the exponential (torch.exp(output)). With log_softmax you would need to use the NLLLoss (Negative likelihood loss).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#29012022\n",
    "\n",
    "Inherence is the process of making predictions.\n",
    "\n",
    "Pytorch notes:\n",
    "\n",
    "model.eval() is used to turn off dropout when doing validation\n",
    "Once one epoch is done, turn back on the dropout by using model.trai(). Always remember to do the whole validation with no_grad. \n",
    "\n",
    "To save and load a model in Pytorch it is important to know that when loading it back to another model, it will need to have the same architecture of the one saved.\n",
    "When saving the model, you can create a checkpoint dictionary:\n",
    "\n",
    "```\n",
    "checkpoint = {'input_size': 784,\n",
    "              'output_size': 10,\n",
    "              'hidden_layers': [each.out_features for each in model.hidden_layers],\n",
    "              'state_dict': model.state_dict()}\n",
    "```\n",
    "then save the model\n",
    "torch.save(checkpoint, 'checkpoint.pth')\n",
    "\n",
    "To load it back you can create a custom function to do that. Example:\n",
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath) => Load the pth file\n",
    "    model = fc_model.Network(checkpoint['input_size'], => Create the model based on the checkpoint content\n",
    "                             checkpoint['output_size'],\n",
    "                             checkpoint['hidden_layers'])\n",
    "    model.load_state_dict(checkpoint['state_dict']) => Load state dict\n",
    "    \n",
    "    return model\n",
    "    \n",
    "\n",
    "Then you can just simply do:\n",
    "\n",
    "model = load_state_dict('checkpoint.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#30012022\n",
    "\n",
    "Convolutional Neural Networks CNN\n",
    "\n",
    "Data Normalization is a pre-processing step which ensure that each input (in case of images), so each pixel is coming with a standard distribution. For example in MNIST hand-written numbers dataset the values of the pixelf would be from 0 to 255 (Black and white pics). However since the net workds with gradients we normalize the data to be between 0 and 1.\n",
    "\n",
    "Data normalization is typically done by substracting the mean of the pixel values from each pixel and then dividing the result by the standard deviation of all the pixel values. \n",
    "\n",
    "The distribution of this data will resemble of a Gaussian function. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#31012022\n",
    "\n",
    "CrossEntropyLoss in Pytorch has two steps, softmax and negative log likelihood loss.\n",
    "If using CrossEntropyLoss as criterion, then you do not need to specify the softmax as output layer.\n",
    "However, it is possible to alternatively define a log softmax layer in the model architecture and then use the NLLLoss as criterion. \n",
    "\n",
    "When building a model, we should always use 3 dataset, training, validation and testing.\n",
    "The validation is used while training but it is used to verify how well the model can generalize. With the validation set we do not update weights.\n",
    "The number of epochs to be used should be when the validation loss starts increasing and training loss keeps decreasing. That is when the model starts overfitting. \n",
    "The data for test is data has never been seen before by tbe model and should be used to verify how accurate the model is. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#01022022\n",
    "\n",
    "Differences between MLPs and CNNs\n",
    "\n",
    "MLPs use only fully connected layers while CNNs also use sparsely connected layers. In MLPs there is a lot of parameters and even with relatively small picture this number is high, resulting in higher computational needs. \n",
    "\n",
    "MLPs only accepts vectors (flattening the image) while CNNs also accept matrices. The fact of flattening an image can lose some information about the position of the pixels within the image.\n",
    "\n",
    "\n",
    "Convolutional Layer = Convolutional Kernel\n",
    "\n",
    "Spatial patterns in an image are related to either color or shape.\n",
    "\n",
    "Filters: to detect changes in intensity in an image, we use filters that look at groups of pixels and react to alternative patterns of dark/light pixels.These filters produce an output that shows edges of an object and differing texture. \n",
    "\n",
    "In images there is a rate of change. Similarly to audio there is high frequency and low frequency. \n",
    "\n",
    "Most images have both high-frequency and low-frequency components. high-frequency image pattern; this part changes very rapidly from one brightness to another. Instead where there is a  change very gradually, which is considered a smooth, low-frequency pattern.\n",
    "\n",
    "<img src='pics/high_low.png'>\n",
    "\n",
    "\n",
    "High pass  filters are applied to images to detect edges. Edges are areas in image where the intensity changes very quickly and very often indicates object boundaries. \n",
    "\n",
    "<img src='pics/Edge_detection_filter.png'>\n",
    "The sum of the matric has to be zero. If the result is positive the image will get brighter, if negative darker. Note that the values in orange summed are the value in the middle. \n",
    "<img src='pics/Convolutional_kernel.png'>\n",
    "\n",
    "Convolution is represented by asteriks, not multiplication\n",
    "\n",
    "<img src='pics/Convolutional_kernel_applied.png'>\n",
    "\n",
    "To find and enhancing an horizontal edge and line you would want to use for example the below kernel:\n",
    "\n",
    "-1 -2 -1\n",
    " 0  0  0\n",
    " 1  2  1\n",
    "\n",
    " This way you can see the difference between bottom and top\n",
    "\n",
    " Sobel filter is commonly used for edge detection and finding patterns in an image. Using the Sobel filter to an image is a way to taking an approximation of the derivative of the image in the x or y direction. \n",
    "\n",
    "\n",
    "Sx = [[-1 0 1],\n",
    "      [-2 0 2],\n",
    "      [-1 0 1]]\n",
    "\n",
    "Sy = [[-1  -2  -1],\n",
    "      [ 0   0   0],\n",
    "      [ 1   2   1]]\n",
    "\n",
    "\n",
    "\n",
    "A basic CNNs has the below structure:\n",
    "\n",
    "<img src='pics/CNN.png'>\n",
    "\n",
    "An image is passed through a filter, in this case 4 convolutional kernels which produce four differently filtered images which are then stacked together. This is the convolutional layer. \n",
    "\n",
    "<img src='pics/CNN2.png'>\n",
    "<img src='pics/CNN3.png'>\n",
    "\n",
    "Grey scale picture are 2D array, height and width.\n",
    "Colored pictures (RGB) are 3D array, height, width and depth. For RGB the depth is 3. \n",
    "A stack of 3 2D matrices. \n",
    "\n",
    "\n",
    "If you take a picture and apply 4 filters, both for detecting horizontal and vertical edges, there will be 4 collections of nodes, called also as feature maps or activation maps:\n",
    "\n",
    "<img src='pics/FeatureMaps.png'>\n",
    "\n",
    "<img src='pics/FeatureMaps2.png'>\n",
    "\n",
    "Note that in the picture above you can see the first two filters are discovering vertical edges. The first one has lighter features on the right, the second on the left. \n",
    "\n",
    "Edges and images appear as a line of lighter pixel next to darker pixels. \n",
    "\n",
    "Also on colored picture you apply filters which in this case are 3D as well, a stack of 3 2D matrices.\n",
    "\n",
    "<img src='pics/RGB_Convolutional.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#02022022\n",
    "\n",
    "Stride is how many pixel the filters will move when applied. For example if strid is 1, it will move 1 pixel at a time. \n",
    "When applying bigger strides you may end up having the filter going over the boundary of the image. In this case you could either ignore those regions and losing part of the information from the image or using padding so , adding 0s to the regions that were not covered by the filter. \n",
    "\n",
    "FILTER APPLIED (2x2):\n",
    "<img src='pics/Padding.png'>\n",
    "\n",
    "Ignore the regions not covered by the filter:\n",
    "\n",
    "<img src='pics/Ignore.png'>\n",
    "\n",
    "Or you could apply a padding by adding zeroes so that you retain all the information:\n",
    "\n",
    "<img src='pics/Padding2.png'>\n",
    "\n",
    "\n",
    "Pooling Layers:\n",
    "\n",
    "Pooling is the layer that takes in aa input a convolutional layer and returns as output the same number of feature maps but reduced. \n",
    "There are different pooling, e.g. max pooling and avg pooling. \n",
    "\n",
    "In Max pooling, once assigned a window size and a stride, the pooling will select the max number within the window:\n",
    "\n",
    "1 9 6 4\n",
    "5 4 7 8\n",
    "5 1 2 9\n",
    "6 7 6 0\n",
    "\n",
    "You start from left top and you would have\n",
    "1 9\n",
    "5 4\n",
    "\n",
    "In this case it takes 9\n",
    "\n",
    "Stride is 2 so next will be\n",
    "6 5\n",
    "7 8\n",
    "\n",
    "In this case take 8\n",
    "\n",
    "Repeat this and you get\n",
    "\n",
    "9 8\n",
    "7 9\n",
    "\n",
    "So from a 4x4 it is now a 2x2. \n",
    "\n",
    "The average pooling instead of return an avg of the window size instead of the max. However in image classification, max pooling is more effective and notices better edges, without missing details. \n",
    "\n",
    "<img src='pics/MaxPooling.png'>\n",
    "\n",
    "<img src='pics/MaxPooling2.png'>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#03022022\n",
    "\n",
    "Remember that pooling throws away some information of an image which may cause some issues, for example in Face recognition.\n",
    "An alternative to pooling is Capsule Networks. Capsule Networks provide a way to detect parts of objects and represent spatial relationships between them. Capsule networks are made of parent and child nodes that build up a complete picture of an object.\n",
    "\n",
    "<img src=\"pics/Capsule_Net.png\">\n",
    "\n",
    "Capsule are basically a collection of nodes, each one containing information about a specific part of the object, e.g. width, orientation, color etc.\n",
    "\n",
    "Each capsule outputs a vector with some magnitude and orientation:\n",
    "\n",
    "Magnitude (m) is the probability that a part exists, a value between 0 and 1.\n",
    "Orientation (θ, theta) is the state of the part properties. \n",
    "To check this repo: https://github.com/cezannec/capsule_net_pytorch\n",
    "\n",
    "\n",
    "Depth: Colored pics have 3, Black and white 1. \n",
    "When working with images, we need to resize them before doing anything. The resizing typically means means to resize the pics as a square. \n",
    "All pics have to have a fixed size also for CNNs with spatial dimension equal to a power of 2 or else a number that is divisible by a large power of 2.\n",
    "\n",
    "How to define a conv layer in Pytorch, example:\n",
    "\n",
    "self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "First 3 is the depth of the image, so if a color image it will be 3\n",
    "16 is for how many filters we want to have\n",
    "The second 3 is the kernel_size, so in this case a 3x3. Typically it can be from 2x2 to 7x7 or up for bigger pics.\n",
    "padding=1 is to make sure we do not loose information from the borders of the pics\n",
    "\n",
    "<img src=\"pics/Conv_Layers.png\">\n",
    "\n",
    "Note that for each conv layer, the number of filters is doubling, and so is the depth. Depth is the filter number of the prev layer.\n",
    "\n",
    "Next is the addition of Pooling so that the dimensions of x, y get cut in half in the case of using a kernel and stride of 2 and 2:\n",
    "\n",
    "<img src=\"pics/PyTorch_MaxPooling.png\">\n",
    "\n",
    "QUESTIONS: \n",
    "\n",
    "How might you define a Maxpooling layer, such that it down-samples an input by a factor of 4? \n",
    "nn.MaxPool2d(4,4) or nn.MaxPool2d(2,4). \n",
    "Since the stride is 4 you would downsample by 4 but it is best to go for 4,4 so that the maxpooling function sees every input pixel once.\n",
    "\n",
    "If you want to define a convolutional layer that is the same x-y size as an input array, what padding should you have for a kernel_size of 7? (You may assume that other parameters are left as their default values.)\n",
    "\n",
    "padding=3\n",
    "\n",
    "E.g. if your pic is 32x32 and kernel is 7x7, after 4 iterations, we are at pixel 28, the kernel will overlay 3 columns that do not exist. \n",
    "\n",
    "nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)\n",
    "in_channels refers to the depth of an input. For a grayscale image, this depth = 1\n",
    "out_channels refers to the desired depth of the output, or the number of filtered images you want to get as output\n",
    "kernel_size is the size of your convolutional kernel (most commonly 3 for a 3x3 kernel)\n",
    "stride and padding have default values, but should be set depending on how large you want your output to be in the spatial dimensions x, y\n",
    "\n",
    "\n",
    "The number of parameters is the x-y size of the final output times the number of final channels/depth, so if pic is 16*16 and last conv layer was filter 20, then it is 16*16*20\n",
    "\n",
    "<img src=\"pics/CNN_Prediction.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#05022022\n",
    "\n",
    "To compute the output size of a convolutional layer we can obtain it from the below formula\n",
    "\n",
    "(W−F+2P)/S+1\n",
    "\n",
    "So if you had a 7x7 input with a 3x3 kernel, padding 0 and stride of 1, the output would be a 5x5: ((7-3+0)/1)+1=5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#06022022\n",
    "\n",
    "When initializing weights for a model it makes sense to have them based on the amount of nodes we have in input. \n",
    "The more inputs a certain node sees the smaller the weight should be.\n",
    "\n",
    "Good practice is to start your weights in the range of  [−𝑦,𝑦]  where  𝑦=1/sqr.root(n)  is the number of inputs to a given neuron)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#07022022\n",
    "\n",
    "Autoencoder: you can think the process of an image going through convolutional layers as a sort of decompression. Autoencoders compress and decompress data. \n",
    "\n",
    "<img src='pics/Autoencoders.png'>\n",
    "\n",
    "An autoencoder has two parts. Encoder that compresses the data and a decoder that reconstruct the data from the compressed one. This is also used in for example GANs.\n",
    "They work well for de-noising and also filling in missing data. \n",
    "\n",
    "<img src='pics/DenoiseImageTrans.png'>\n",
    "\n",
    "The middle layer is the compressed representation of the input from which you can reconstruct the data. Key aspect of an autoencoders is that it can compresse the data such that the content is still maintained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#08022022\n",
    "\n",
    "A simple MLP autoencoder does the job on images like MNIST but a better solution is to use convolutional neural net. \n",
    "The first portion, the encoder, is a stack of convolutional layers and pooling. The second portion, the decoder should reverse the process done by the encoder, it should upscale the image. \n",
    "One way of doing this is called nearest neighbor where pixels are replicated, for example:\n",
    "\n",
    "1 2           1 1 2 2\n",
    "3 4           1 1 2 2\n",
    "              3 3 4 4 \n",
    "\n",
    "However this is just duplicating values and in large images, there are more details and this approach would not work. \n",
    "A better solution to this is transponse convolutional layer.\n",
    "It is important to keep in mind that the stride of the kernel is approximately the ratio of input to ouput dimensions. \n",
    "<img src='pics/StrideRatio.png'>\n",
    "\n",
    "The transpose takes for example the first pixel and places a kernel of 3x3 on it, multiplies that pixel by the kernel weights to get a 3x3 area:\n",
    "<img src='pics/MultipliedKernel.png'>\n",
    "\n",
    "Then with stride 2 we know that the center of the second pixel is 2 positions to the right:\n",
    "\n",
    "<img src='pics/SecondPixel.png'>\n",
    "\n",
    "So the two areas will overlap and the area with overlapping values will be summed up:\n",
    "\n",
    "<img src='pics/SumOverlap.png'>\n",
    "\n",
    "The final outputput would be after doing all 4 pixels a 5x5 area:\n",
    "\n",
    "<img src='pics/AutoencoderOutput.png'>\n",
    "\n",
    "Generally you would use a kernel of 2x2 and a stride of 2 to double the x, y dimensions. \n",
    "\n",
    "<img src='pics/Kernel2Stride2.png'>\n",
    "\n",
    "\n",
    "Good article to read about deconvolution: https://distill.pub/2016/deconv-checkerboard/\n",
    "\n",
    "It is important to note that transpose convolution layers can lead to artifacts in the final images, such as checkerboard patterns.\n",
    "These artifacts can be avoided by resizing the layers using nearest neighbor or bilinear interpolation (upsampling) followed by a convolutional layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#10022022\n",
    "\n",
    "Style Transfer is the ability to apply a style of one image to a content of another image. For example:\n",
    "\n",
    "<img src=\"pics/StyleTransfer.png\">\n",
    "\n",
    "An example for applying style transfer is with VGG19: https://arxiv.org/pdf/1409.1556.pdf\n",
    "\n",
    "The VGG19 CNN has 5 pooling layers and each convolutional layer is a stack of layers:\n",
    "\n",
    "<img src=\"pics/VGG19_ConvLayers.png\">\n",
    "\n",
    "To obtain the target image, the one with content from one image and style from another, we pass both content and style image through the VGG19.\n",
    "\n",
    "When passing the content image, the image will go through till the deepest conv layer, where a representation of the image will be the output. \n",
    "\n",
    "<img src=\"pics/Content_Representation.PNG\">\n",
    "\n",
    "Next we pass the style image and the network will extract different features from multiple layers that represent the style of that image.\n",
    "\n",
    "<img src=\"pics/StyleRepresentation.png\">\n",
    "\n",
    "The output will be the merging of content representation and style representation.\n",
    "\n",
    "The challenge is to get the target image which can start as a blank canvas or a copy of the input image and need to manipulate the style. \n",
    "\n",
    "For the content representation, in the paper, the output is takem from the conv layer 4_2. The output is then compared to the input image.\n",
    "\n",
    "<img src=\"pics/ContentRepresentationOutput.png\">\n",
    "\n",
    "We need to calculate a content loss which is the mean square loss between the content representation from image and content representation of the  target image.\n",
    "\n",
    "<img src=\"pics/ContentLoss.png\">\n",
    "\n",
    "This will measure how far apart are the two representation from each other. The aim is to minimize the loss by backpropagation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#12022022\n",
    "\n",
    "Similarly to comparing the content representation and target rapresentation, we do for the style where we need to compare the style representation of the style image and the target image. \n",
    "We basically check how similar features are in the same layer of the network. Similarities will be colors and textures.\n",
    "\n",
    "<img src='pics/MultiScaleRep.png'>\n",
    "\n",
    "By including the correlations of between multi layers of different sizes we can obtain a multi scale style representation of the style image where lareg and small style features are caught. \n",
    "\n",
    "The gram matrix defines the correlations. \n",
    "\n",
    "<img src='pics/grammatrix.png'>\n",
    "\n",
    "If we have a 4x4 pic passed through the conv layer with 8 of depth, the matrix will have 8 feature maps that we want to find the relationships between. \n",
    "\n",
    "We basically vectorize (flatten the values).First row in the feature map are the first 4 slot of the vector.\n",
    "\n",
    "<img src='pics/FlattenGram.png'>\n",
    "\n",
    "By vectorizing the feature maps we transfor a 3D conv layer into a 2D matrix of values \n",
    "\n",
    "<img src='pics/ConversionGram.png'>\n",
    "\n",
    "Next we need to multiply this matrix by the Transponse of the matrix\n",
    "\n",
    "<img src='pics/TransposeGram.PNG'>\n",
    "\n",
    "The result will be a 8 by 8 matrix:\n",
    "\n",
    "<img src='pics/FinalGram.png'>\n",
    "\n",
    "So for example the value in row 4, column 2, will hold the similarities of the fourth and second feature maps in the layer. \n",
    "Gram matrix is one of the mostly used in practice when doing style. \n",
    " \n",
    "\n",
    "\n",
    "QUESTIONS:\n",
    "\n",
    "Given a convolutional layer with dimensions d x h x w = (20*8*8), what length will one row of the vectorized convolutional layer have? (Vectorized means that the spatial dimensions are flattened.)\n",
    "\n",
    "64 => multiply h x w since it is a 2D vector\n",
    "\n",
    "\n",
    "Given a convolutional layer with dimensions d x h x w = (20*8*8), what dimensions (h x w) will the resultant Gram matrix have?\n",
    "\n",
    "20*20 => The gram matrix will have the number of rows and columns as the depth of the conv layer. \n",
    "\n",
    "Same as for content, the style loss is calculated. At each of the 5 layers by comparing target vs style image. We only change the target image.\n",
    "\n",
    "The total loss is basically the sum of the content loss and style loss. We reduce the loss by backpropagation.\n",
    "\n",
    "<img src='pics/TotalLoss.PNG'>\n",
    "\n",
    "However since the style loss and content loss are calculated differently we need to apply constant weights to each. Normally the constant used for style is much larger than the one for content. \n",
    "\n",
    "<img src='pics/ConstantsAlfaBeta.png'>\n",
    "\n",
    "Normally we talk about alfa over beta as ratio.\n",
    "Example of ratio: 1/10\n",
    "\n",
    "<img src='pics/ExampleRatio.png'>\n",
    "\n",
    "In this example we can see there is much of content but not a lot of style. When increasing the beta, the target image starts having more and more style. \n",
    "\n",
    "<img src='pics/IncreaseBeta.png'>\n",
    "\n",
    "Until a certain point the style is overclassing content. 10 to the -4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#15022022\n",
    "\n",
    "From Wikipedia:\n",
    "Precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances, while recall (also known as sensitivity) is the fraction of relevant instances that were retrieved. Both precision and recall are therefore based on relevance.\n",
    "\n",
    "Consider a computer program for recognizing dogs (the relevant element) in a digital photograph. Upon processing a picture which contains ten cats and twelve dogs, the program identifies eight dogs. Of the eight elements identified as dogs, only five actually are dogs (true positives), while the other three are cats (false positives). Seven dogs were missed (false negatives), and seven cats were correctly excluded (true negatives). The program's precision is then 5/8 (true positives / selected elements) while its recall is 5/12 (true positives / relevant elements).\n",
    "\n",
    "\n",
    "Sensitivity and specificity are not the same as precision and recall\n",
    "\n",
    "Sensitivity: Of all the people with cancer, how many were correctly diagnosed?\n",
    "\n",
    "Specificity: Of all the people without cancer, how many were correctly diagnosed?\n",
    "\n",
    "And precision and recall are the following:\n",
    "\n",
    "Recall: Of all the people who have cancer, how many did we diagnose as having cancer?\n",
    "Precision: Of all the people we diagnosed with cancer, how many actually had cancer?\n",
    "From here we can see that Sensitivity is Recall, and the other two are not the same thing.\n",
    "\n",
    "<img src='pics/confusion-matrix.png'>\n",
    "\n",
    "Sensitivity = {TP}/{TP + FN}\n",
    "​\n",
    "and\n",
    "\n",
    "Specificity = {TN}/{TN + FP}\n",
    "\n",
    "<img src='pics/precision-recall.png'>\n",
    "\n",
    "ROC Curves => Receiver Operating Characteristic curve is used to evaluate a model. \n",
    "\n",
    "<img src='pics/ROC.png'>\n",
    "\n",
    "When calculating and plotting the data you calculate the True Positive and False Positive Rate\n",
    "\n",
    "True Positive Rate = True Pos/All Pos\n",
    "False Positive Rate = False Pos/All Negative\n",
    "\n",
    "<img src='pics/TPFPRatio.png'>\n",
    "\n",
    "You can keep calculating at each split and get all coordinates and plot them to a graph. This will give you the ROC. \n",
    "\n",
    "Below an example of ROC for the skin cancer detection (note that it is flipped):\n",
    "\n",
    "<img src='pics/ROC_SkinCancer.png'>\n",
    "\n",
    "https://www.nature.com/articles/nature21056.epdf?author_access_token=8oxIcYWf5UNrNpHsUHd2StRgN0jAjWel9jnR3ZoTv0NXpMHRAJy8Qn10ys2O4tuPakXos4UhQAFZ750CsBNMMsISFHIKinKDMKjShCpHIlYPYUHhNzkn6pSnOCt0Ftf6\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#16022022\n",
    "\n",
    "Confusion Matrix:\n",
    "\n",
    "Model if patient is heatlhy or sick\n",
    "\n",
    "If sick and diagnosed sick => True Positive\n",
    "\n",
    "If Sick and Diagnosed Healthy => False Negative\n",
    "\n",
    "If Heatlhy and Diagnosed Sick => False Positive\n",
    "\n",
    "If Healthy and Diagnosed Healthy => True Negative\n",
    "\n",
    "Type 1 => False Positive (When heatlhy was diagnosed sick)\n",
    "\n",
    "Type 2 => False Negative (When sick was diagnosed heatlhy)\n",
    "\n",
    "In a confusion matrix, entries must be between 0 and 1. The sum of every row must be 1.\n",
    "The confusion matrix can be also more than just 2x2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#22022022\n",
    "\n",
    "Recurrent Neural Network or RNN are artificial neural network that have memory that can capture temporal dependencies which are dependencies over time.\n",
    "In neural networks like CNN there is a lack of memory while RNN uses memory when producing an output.\n",
    "\n",
    "Year 1989, The first RNN was the TDNN ot Time Delay Neural Network. Where inputs from past timesteps were introduced to the network input changing the actual external inputs allowing to look beyond the current timestep but also limited to the window selected. \n",
    "\n",
    "Year 1990, Simple RNN or Elman Network was introduced. However these RNN suffered from a flaw that goes by the name of vanishing gradient. Basically when training the network we do backpropagation which adjusts the weight matrices with the gradient. During this process we continuosly multiply derivaties and the values may be so small that the gradient will vanish. \n",
    "\n",
    "CHAIN RULE REMINDER\n",
    "\n",
    "<img src='pics/chainrule.png'>\n",
    "\n",
    "\n",
    "Backpropagation is just a calculation of partial derivatives.\n",
    "\n",
    "BACKPROPAGATION REMINDER\n",
    "\n",
    "What is the update rule of weight matrix W1. What is the partial derivative of y with respect to W1. \n",
    "\n",
    "<img src='pics/BackpropReminder.png'>\n",
    "<img src='pics/Derivatives.png'>\n",
    "\n",
    "C as there are two separate paths for which W1 contributes to:\n",
    "\n",
    "<img src='pics/Path1.png'>\n",
    "<img src='pics/Path2.png'>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***2602202***\n",
    "\n",
    "RNNs are based on similar principles as feedforward network (FFNs) however there are two main differences between FFNs and RNNs:\n",
    "\n",
    "Sequences as input in training phase and memory elements (States)\n",
    "\n",
    "Memory is defined as the output of hidden layer neurons which will be used as additional input to the network during next training step. \n",
    "\n",
    "The basic three layer neural network with feedback that serve as memory input is called simple RNN or Elman Network. \n",
    "\n",
    "<img src='pics/Elman.png'>\n",
    "\n",
    "<img src='pics/Unfolded.png'>\n",
    "\n",
    "In RNNs the output depends not only on the input of the current input and weights but also previous inputs, as shown above. \n",
    "\n",
    "<img src='pics/FoldedModel.png'>\n",
    "\n",
    "In FFNs the hidden layer depended solely on the current input and weights as well as an activation function \\PhiΦ\n",
    "In RNNs the state layer depends on the current inputs, weights, activation function and also on the previous state:\n",
    "\n",
    "<img src='pics/EquationRNN.png'>\n",
    "\n",
    "The ouput vector is calculate the same way as for FFNN which can be a linear combination of inputs to each output node with corresponding matrix Wy or a softmax function of the same combination.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***27022022***\n",
    "\n",
    "EXAMPLE OF SEQUENCE DETECTOR, WORD DETECTOR\n",
    "\n",
    "Word to detect: Udacity\n",
    "\n",
    "We can use a one-hot vector encoding as input. This vector will contain 7 binary values (7 char of the word udacity)\n",
    "\n",
    "The visualization of the vector is like this (Note that the order is arbitrary):\n",
    "\n",
    "<img src='pics/OneHotVector.PNG'>\n",
    "\n",
    "Other letters would have just zeroes. The word udacity would be like this:\n",
    "\n",
    "<img src='pics/Udacityword.PNG'>\n",
    "\n",
    "We can feed the network by giving random letters at each timestep.\n",
    "Occasionally we will insert the word udacity too. The target will be always 0 except for the last letter of the word udacity. \n",
    "\n",
    "<img src='pics/UdacityWord Target.PNG'>\n",
    "\n",
    "After training the system and optimizing the weights the network would ouput that a sequence is detected when for example a value close to 1 is found, e.g.:\n",
    "<img src='pics/WordDetected.PNG'>\n",
    "\n",
    "The threshold is arbitrary.\n",
    "\n",
    "RNN can deal with varying sequence lengths.\n",
    "\n",
    "When training RNNs we use backpropagation like for FFNNs but with a variation. For RNNs we need to consider previous time steps as the system has memory. This process is called Backpropagation Through time BPTT.\n",
    "\n",
    "As loss function, we use MSE mean squared error. \n",
    "\n",
    "In BPTT we train the network at timestep t as well as take into account all of the previous timesteps.\n",
    "\n",
    "To update matrix Ws we do the following, if we were at t=3:\n",
    "\n",
    "<img src='pics/FormulaUpdateStateFull.png'>\n",
    "\n",
    "In brief:\n",
    "\n",
    "<img src='pics/FormulaUpdateState.png'>\n",
    "\n",
    "When calculating the partial derivative of the Loss Function with respect to Ws, we need to consider all of the states contributing to the output.In BPTT we will take into account every gradient stemming from each state, accumulating all of these contributions.\n",
    "\n",
    "\n",
    "Fo update the matrix Wx instead we do the following ,if we were at t =3:\n",
    "\n",
    "<img src='pics/WxFormulaFull.png'>\n",
    "\n",
    "In brief:\n",
    "\n",
    "<img src='pics/WxFormula.png'>\n",
    "How to update the weight matrix U at time t+1 (over 2 timesteps) ?\n",
    "<img src='pics/Question3BPTT.png'>\n",
    "<img src='pics/Question3BPTT2.png'>\n",
    "<img src='pics/Question3BPTT3.png'>\n",
    "<img src='pics/Question3BPTT4.png'>\n",
    "<img src='pics/Question3BPTT5.png'>\n",
    "<img src='pics/Question3BPTT6.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***28022022***\n",
    "\n",
    "Uo to 8-10 timesteps, the BPTT works fine, otherwise we start having the so called vanishing gradient problem. \n",
    "To obviate this, LSTM (Long short-term memory) come into play. \n",
    "\n",
    "Another problem can be exploding gradient, where the gradient grows uncontrollably. A method use to obviate this problem is called gradient clipping. With gradient clipping we check at each timestep if the gradient is over a threshold. When it goes over, the gradient is normalized. This penalizes very large values. \n",
    "\n",
    "Algorithm 1 Pseudo-code for norm clipping the gradients whenever they explode\n",
    "\n",
    "<img src='pics/ExplodingGradient_Clipping.png'>\n",
    "\n",
    "\n",
    "\n",
    "***LSTM***\n",
    "\n",
    "There is a fundamental difference between a RNN neuron and a LSTM cell:\n",
    "<img src='pics/RNN_Neuron.png'>\n",
    "\n",
    "LSTM:\n",
    "\n",
    "<img src='pics/LSTM_Cell.png'>\n",
    "\n",
    "LSTM does not have basic computations like in RNNs. LSTM cells have 4 different calculations. \n",
    "\n",
    "<img src='pics/LSTM_Cell_Functions.png'>\n",
    "\n",
    "LSTM Cell can learn over many timesteps, over a 1000.\n",
    "\n",
    "Fully differentiable, meaning that each functions have a gradient or derivative that we can calculate. \n",
    "Functions are, sigmoid, hyperbolic tangent, multiplication and addition. \n",
    "This allows us to use backpropagation or SGD when updating the weights.\n",
    "LSTM can decide which info to remove or forget, which to store and when to use it. \n",
    "There are 3 sigmoid whose outputs are between 0 and 1. Let's\n",
    "All data passes through if around 1 and do not pass through if around 0. \n",
    "These sigmoid functions decide on:\n",
    "\n",
    "- What goes into the cell \n",
    "- What retains within the cell \n",
    "- What passes to the output\n",
    "\n",
    "Gating functions that are trained to with backpropagation. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***01032022***\n",
    "\n",
    "LSTM have 4 gates:\n",
    "\n",
    "Forget Gate\n",
    "Remember Gate\n",
    "Learn Gate\n",
    "Use Gate\n",
    "\n",
    "LSTM work with long term and short term memory.\n",
    "\n",
    "<img src='pics/LSTM_gates.png'>\n",
    "\n",
    "Long term memory goes to the Forget Gate where it forgets everything that does not consider useful.\n",
    "\n",
    "Short term memory and the event go together to the learn gate containing the information we have recently learnt and removes any uncecessary information.  \n",
    "\n",
    "The long term memory that was not forgotten and the new learnt info are joined together and go to the remember gate. This will output an updated New Long Term Memory\n",
    "\n",
    "Use Gate takes in the same as the remember gate and outputs both prediction and new short term ouput\n",
    "\n",
    "<img src='pics/LSTM_Gates_1.PNG'>\n",
    "\n",
    "<img src='pics/LSTM_Gates_2.PNG'>\n",
    "\n",
    "<img src='pics/LSTM_Gates_3.PNG'>\n",
    "\n",
    "<img src='pics/LSTM_Gates_4.PNG'>\n",
    "\n",
    "In LSTM we get LTM and STM, we get an event and output out of the LSTM and then this passed to next nodes. \n",
    "\n",
    "<img src='pics/LSTM_LTMSTM.png'>\n",
    "\n",
    "\n",
    "\n",
    "#***THE LEARN GATE***\n",
    "\n",
    "We have a Long Term Memory which for this example will be a nature/science program. \n",
    "A short term memory which is a squirrel and a tree. \n",
    "An event which is either a dog or a wolf. \n",
    "\n",
    "The learn gate will take the short term memory and the event and combines them and then it ignores part of it to keep the important part of it. So in our case it will forget that there was a tree and keep the squirrel and the wolf.\n",
    "\n",
    "Mathematically it works this way:\n",
    "\n",
    "We have the STM at t-1 and the Event E at time t\n",
    "We combine these into a linear function which consists of joining their vectors STMt-1 and Et multiplied by a matrix, adding a bias and then squish the results via tanh. This is Nt.\n",
    "\n",
    "We ignore part of this by multipying Nt with an ignore factor i of t.\n",
    "\n",
    "We calculate the i of t by combining STMt-1 and Et multiplied by another matrix and added another bias and squish this result with sigmoid. \n",
    "\n",
    "<img src='pics/LearnGate.png'>\n",
    "\n",
    "<img src='pics/LearnGateEquations.png'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***02032022***\n",
    "\n",
    "#***FORGET GATE***\n",
    "\n",
    "Continuing with the nature/science show example, the forget gate takes in the Long Term Memory and decides what parts to keep and to forget. It forgets about the science part. \n",
    "\n",
    "The output of the Forget gate is LTMt-1 multiplied by a factor ft which is calculated as below:\n",
    "\n",
    "<img src='pics/Equation_ForgetGate.png'>\n",
    "\n",
    "<img src='pics/ForgetGate.png'>\n",
    "\n",
    "It is a one small layer neural network with a linear function combined with activation function sigmoid.\n",
    "\n",
    "We take the short term memory t-1 and Event Et, multiplied by a matrix an added a bias. All this squished by sigmoid.\n",
    "\n",
    "#***REMEMBER GATE***\n",
    "\n",
    "This is the simplest one.\n",
    "We basically sum LTMt-1*ft+Nt*it. Essentially the outputs of the Learn and Forget gates are added in the Remember Gate. \n",
    "\n",
    "<img src='pics/Equation_RememberGate.PNG'>\n",
    "\n",
    "<img src='pics/RememberGate.png'>\n",
    "\n",
    "#***USE GATE***\n",
    "\n",
    "In the use gate the LTM coming from the Forget Gate and the short term memory from the Learn Gate to come up with a new STM and output these are the same thing. \n",
    "\n",
    "It applies a small neural network on the output of the Forget Gate (LTM) by using a tanh activation function. \n",
    "Then on the STM and Event coming from the Learn Gate it applies a small neural network by applying a sigmoid.\n",
    "\n",
    "The final step the two outputs are multiplied and obtain a LTM which is also used as STM. \n",
    "\n",
    "<img src='pics/UseGate.png'>\n",
    "\n",
    "<img src='pics/Equation_UseGate.png'>\n",
    "\n",
    "Recap:https://www.youtube.com/watch?v=IF8FlKW-Zo0\n",
    "\n",
    "<img src='pics/LSTM_Cell_Detail.PNG'>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#***03032022***\n",
    "\n",
    "Say you've defined a GRU layer with input_size = 100, hidden_size = 20, and num_layers=1. What will the dimensions of the hidden state be if you're passing in data, batch first, in batches of 3 sequences at a time?\n",
    "\n",
    "(1, 3, 20) => The hidden state should have dimensions: (num_layers, batch_size, hidden_dim)\n",
    "\n",
    "\n",
    "One of the most difficult part for RNN is to get the batches right. With RNNs we are training on sequences of data, like text, audio, numbers etc.\n",
    "\n",
    "However if you were to split the sequence you would take advantage of matrix operations to make the training more efficient as the RNN is training on multiple sequences in parallel.\n",
    "\n",
    "For example lets say you have a sequence like this\n",
    "\n",
    "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "\n",
    "You could feed this as 1 sequence but what is better is if you split this into two sequences\n",
    "\n",
    "[1, 2, 3, 4, 5, 6]\n",
    "[7, 8, 9, 10, 11, 12]\n",
    "\n",
    "In this case we would have a batch size of 2 and we can choose the lenght of the sequence to feed to the network, e.g. it could be sequence length of 3 so you would feed the first three numbers of each batch [1, 2, 3] & [7, 8, 9]\n",
    "We can retain the hidden state of the previous batch and use it at the start of the next batch, this way the sequence information is transferred across batches for each mini sequences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***#06032022***\n",
    "\n",
    "Hyperparameters can be divided into two categories:\n",
    "\n",
    "- Optimizer hyperparameters, related to optimazation and training process, such as Learning Rate, Minibatch size and Epoch\n",
    "\n",
    "- Model Hyperparameters, such as number of layers and hidden units and model specific hyperparameter\n",
    "\n",
    "LEARNING RATE\n",
    "\n",
    "The learning rate is the most important Optimizer hyperparameter. A good starting point is 0.01\n",
    "\n",
    "Possible good candidates are below:\n",
    "\n",
    "0.1\n",
    "0.01\n",
    "0.001\n",
    "0.0001\n",
    "0.00001\n",
    "0.000001\n",
    "\n",
    "Depending on the behaviour of the neural network training. Ideally in a pefect case the relationship between weights and Error value is a U shaped curve. \n",
    "\n",
    "<img src='pics/LearningRate.png'>\n",
    "\n",
    "A good technique to decrease Learning Rate while training is the Learning Rate Decay where you can either decrease the LR linearly, e.g. decrease it by half every x epoch or decrease it exponentionally every x epoch.\n",
    "\n",
    "Besides decreasing every a fix amount of epoch there are clever algorithms that are capable of both decreasing and increasing LR depending on the training behaviour, so called adaptive learning rate. \n",
    "\n",
    "\n",
    "Say you're training a model. If the output from the training process looks as shown below, what action would you take on the learning rate to improve the training?\n",
    "\n",
    "Epoch 1, Batch 1, Training Error: 8.4181\n",
    "Epoch 1, Batch 2, Training Error: 8.4177\n",
    "Epoch 1, Batch 3, Training Error: 8.4177\n",
    "Epoch 1, Batch 4, Training Error: 8.4173\n",
    "Epoch 1, Batch 5, Training Error: 8.4169\n",
    "\n",
    "\n",
    "Increase LR as it is learning too slow.\n",
    "\n",
    "\n",
    "\n",
    "Say you're training a model. If the output from the training process looks as shown below, what action would you take on the learning rate to improve the training?\n",
    "\n",
    "Epoch 1, Batch 1, Training Error: 8.71\n",
    "Epoch 1, Batch 2, Training Error: 3.25\n",
    "Epoch 1, Batch 3, Training Error: 4.93\n",
    "Epoch 1, Batch 4, Training Error: 3.30\n",
    "Epoch 1, Batch 5, Training Error: 4.82\n",
    "\n",
    "\n",
    "Decrease the LR OR Use adaptive LR\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***#07032022***\n",
    "\n",
    "***Minibatch*** size is another optimizer hyperparameter that affects the speed of the training process. Too small batch would result in a too slow training and too large would through off memory issues due to computational needs for the training process. A good starting number is 32. Below in red too small on the left and too large on the right:\n",
    "\n",
    "<img src='pics/MinibatchSize.png'>\n",
    "\n",
    "Some studies showed that the difference between 32 to 256 was not much in terms of performances, it did decrease a bit when having a larger batchsize but not much.\n",
    "Paper on this: https://arxiv.org/abs/1606.02228\n",
    "\n",
    "\n",
    "***Epochs*** is the number of iterations we run the training process. It is hard to establish a correct amount right off the bat but a good technique that can be used is Early stopping whihc allows us to stop the training and avoid overfitting. \n",
    "\n",
    "<img src='pics/EarlyStopping.png'>\n",
    "\n",
    "It seems that PyTorch does not have a built in early stopping mechanism so you need to keep track of the validation losses and then store them at each epoch. Whenever the val err is lower than previous then you can save that model. You can check this: https://debuggercafe.com/using-learning-rate-scheduler-and-early-stopping-with-pytorch/\n",
    "\n",
    "\n",
    "***Number of Hidden Units*** is the number of units in the hidden layer. Generally speaking the more the units are, the better. However, if the number of unit is way too large, this could lead to overfitting where the Training Error is much better than the validation Error. If this happens, you want to ***decrease*** the number of units or you could use regularization like dropout or L2 regularization. \n",
    "\n",
    "As number of hidden layers, Andrea Karpathy stated that a 3 hidden layers outperfoms a 2 layer network but more than 3 would actually not be so beneficial. The only exception are CNN where the deeper they are the better they perform generally. \n",
    "\n",
    "\"in practice it is often the case that 3-layer neural networks will outperform 2-layer nets, but going even deeper (4,5,6-layer) rarely helps much more. This is in stark contrast to Convolutional Networks, where depth has been found to be an extremely important component for a good recognition system (e.g. on order of 10 learnable layers).\" ~ Andrej Karpathy in https://cs231n.github.io/neural-networks-1/\n",
    "\n",
    "Read (110-120): https://www.deeplearningbook.org/contents/ml.html\n",
    "\n",
    "\n",
    "Practical recommendations for gradient-based training of deep architectures by Yoshua Bengio https://arxiv.org/abs/1206.5533\n",
    "\n",
    "Deep Learning book - chapter 11.4: Selecting Hyperparameters by Ian Goodfellow, Yoshua Bengio, Aaron Courville https://www.deeplearningbook.org/contents/guidelines.html\n",
    "\n",
    "Neural Networks and Deep Learning book - Chapter 3: How to choose a neural network's hyper-parameters? by Michael Nielsen http://neuralnetworksanddeeplearning.com/chap3.html#how_to_choose_a_neural_network's_hyper-parameters\n",
    "\n",
    "Efficient BackProp (pdf) by Yann LeCun http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\n",
    "\n",
    "More specialized sources:\n",
    "\n",
    "How to Generate a Good Word Embedding? by Siwei Lai, Kang Liu, Liheng Xu, Jun Zhao https://arxiv.org/abs/1507.05523\n",
    "Systematic evaluation of CNN advances on the ImageNet by Dmytro Mishkin, Nikolay Sergievskiy, Jiri Matas https://arxiv.org/abs/1606.02228\n",
    "Visualizing and Understanding Recurrent Networks by Andrej Karpathy, Justin Johnson, Li Fei-Fei https://arxiv.org/abs/1506.02078\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***#08032022***\n",
    "\n",
    "Word embedding is a term for models that map certain words or phrases to vectors of numerical values. These vectors are called embeddings. Neural networks can learn to do word embedding. \n",
    "This technique in general is used to reduce the dimensionality of text data but embedding models can also learn interesting trades about words in a vocabulary.\n",
    "For example Word2Vec model learns to map words to embeddings that contain semantic meaning, for example it can learn the relationish beween verb tenses\n",
    "\n",
    "Walking ====> Walked\n",
    "\n",
    "Swimming ====> Swam \n",
    "\n",
    "OR relationship between gender \n",
    "\n",
    "Man =====> King\n",
    "Woman =====> Queen\n",
    "\n",
    "We can think of this embeddings as vectors that represent mathematically the relationship between words in a vocabulary.\n",
    "\n",
    "<img src='pics/Word2VecRelations.png'>\n",
    "\n",
    "The embeddings are learnt by a body of text which means that if there are false info in the text, these will be replicated in the embeddings. De-biased word embedding is actually an active field. \n",
    "\n",
    "\n",
    "***How Word2Vec works***\n",
    "\n",
    "Generally when working with text data, there are thousands of words from a vocabulary. In RNNs we can feed each word as a one-hot encoded vector. This means that we pass a gigantic vector with values of 0s except for the one position set to 1, which represents the word passed currently:\n",
    "\n",
    "<img src='pics/OneHotWord.png'>\n",
    "\n",
    "Then you pass this vector to a hidden layer and the ouput is calculated by multiplying the input by weights, resulting in a huge matrix of values of which most of them are zeroes since most of the inputs were 0. This is also expensive computationally and a waste.\n",
    "\n",
    "<img src='pics/Computational.png'>\n",
    "\n",
    "To solve this problem we can use embeddings. \n",
    "\n",
    "To learn word embedding we use a fully -connected linear layer. We will have an embedding layer which will have a matrix of learnt weights during training.\n",
    "With this matrix of weights we can skip the big multiplication step done before. \n",
    "We will grab the values from the output of our hidden layer directly from a row in the weights matrix.\n",
    "We can do this because the multiplication of a one-hot encoded vector with the weight matrix will return only the row of the matrix that corresponds to the index of the 1 \n",
    "\n",
    "<img src='pics/EmbeddingMatrix.png'>\n",
    "\n",
    "Instead of doing Matrix multiplication, we can use the embedding weight matrix as a lookup table \n",
    "\n",
    "<img src='pics/LookupTable.png'>\n",
    "\n",
    "So instead of using the one-hot encoded vector, we can encode each word as a unique integer\n",
    "\n",
    "<img src='pics/EmbeddingLookup.png'>\n",
    "\n",
    "Papers: https://video.udacity-data.com/topher/2018/October/5bc56d28_word2vec-mikolov/word2vec-mikolov.pdf\n",
    "https://video.udacity-data.com/topher/2018/October/5bc56da8_distributed-representations-mikolov2/distributed-representations-mikolov2.pdf\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#***09032022***\n",
    "\n",
    "Cosine similarity is the cosine of the angle between two n-dimensional vectors in an n-dimensional space. It is the dot product of the two vectors divided by the product of the two vectors' lengths (or magnitudes).\n",
    "The closer the cosine value to 1, the smaller the angle and the greater the match between vectors\n",
    "\n",
    "$$\n",
    "\\mathrm{similarity} = \\cos(\\theta) = \\frac{\\vec{a} \\cdot \\vec{b}}{|\\vec{a}||\\vec{b}|}\n",
    "$$\n",
    "\n",
    "\n",
    "TSNE=> T-Distributed Stochastic Neighbor Embedding is a way to visualize data. TSNE is great at clustering the data. \n",
    "More info on visualization: http://colah.github.io/posts/2014-10-Visualizing-MNIST/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#***11032022***\n",
    "\n",
    "In the SkipGram model we can apply a Negative sampling. Before Negative sampling for every input given to the network, we are making very small changes to millions of weights although we have one true example. This makes the training much slow and inefficient. We can approximate the loss from the softmax layer by updating only a subset of the weights at once. We update the weights of the correct example and only a small number of incorrect/noise examples. This is negative sampling. \n",
    "The trick to use negative sampling is to use another embedding table to mad the hidden layer to the output word. So in the SkipGram we will have two embedding layers, one from input to hidden and the other from hidden to output. \n",
    "Additionally the Loss function needs to be changed and it going to be the below:\n",
    "\n",
    "$$\n",
    "- \\large \\log{\\sigma\\left(u_{w_O}\\hspace{0.001em}^\\top v_{w_I}\\right)} -\n",
    "\\sum_i^N \\mathbb{E}_{w_i \\sim P_n(w)}\\log{\\sigma\\left(-u_{w_i}\\hspace{0.001em}^\\top v_{w_I}\\right)}\n",
    "$$\n",
    "\n",
    "More details in the notebook for SkipGram for Negative Sampling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03212022\n",
    "\n",
    "ATTENTION\n",
    "\n",
    "In Machine learning attention methods gives a mechanism for adding selective focus to be integrated into our machine learning models. Typically one that does it sequentially. \n",
    "Attention was added for example in the Google Translate from 2016. \n",
    "\n",
    "\n",
    "HOW SEQUENCE TO SEQUENCE MODEL WORKS ------------------------\n",
    "\n",
    "Seq2Seq model takes in an input that is a sequence of items, e.g. words and then it produces another sequence of items as output. \n",
    "\n",
    "A sequence to sequence model typically has an encoder and decoder. The input is sequentially given to the encoder creates a representation of this input, as a single vector, also called Context vector. \n",
    "This Conext vector is then passed to the Decoder which will then output the translation if we were to talk about translation model. \n",
    "Both Encoder and Decoder are RNN (LSTMs)\n",
    "\n",
    "The context vector is a vector of numbers representing what the encoder managed to identify from the input given. \n",
    "In a real scenario, this vector can be of 256 of length or 512 and above. \n",
    "\n",
    "<img src='pics/Seq2Seq.PNG'>\n",
    "\n",
    "The constraint of using this conext vector is that it is hard to choose a specific size that would fit all scenarios. One could think to use a large vector all the time to cover all scenarios but then the model would overfit with short sequences, plus performances would be affected when adding parameters. \n",
    "\n",
    "This is what Attention solves. \n",
    "\n",
    "In the Attention model, there is a substantial difference:\n",
    "\n",
    "<img src='pics/Seq2SeqAttention.PNG'>\n",
    "\n",
    "The input is still passed sequentially to the encoder which outputs an hidden state per each item of the sequence. Then instead of combining all into one context vector, all hidden states are passed to the decoder. \n",
    "\n",
    "\n",
    "An attention decoder pays attention to the appropriate part of the input sequence. During training phase the decoder learns to focus on which part of the sequence to work on first. \n",
    "And it is not just stupidly going sequentially one after the other but it will learn sophisticated behaviours. \n",
    "\n",
    "<img src='pics/AttentionTranslation.PNG'>\n",
    "\n",
    "\n",
    "You can see here that the model was going pretty sequentially for every word until the (la) then it jumped to European as the order in English is different than French.  \n",
    "\n",
    "\n",
    "In the decoder with attention we have the following steps happening:\n",
    "\n",
    "<img src='pics/AttentionDecoder_Context_Vector.PNG'>\n",
    "\n",
    "In this case each hidden state is scored and passed to softmax. We then multiply the Hidden state by the softmax scores and sum them up. \n",
    "\n",
    "This create the Context Vector for the decoder. \n",
    "\n",
    "Next the decoding phase:\n",
    "\n",
    "<img src='pics/AttentionDecoder.PNG'>\n",
    "\n",
    "\n",
    "There are two main types of Attention:\n",
    "\n",
    "***Bahdanau or Additive Attention and Luong or Multiplicative Attention.***\n",
    "\n",
    "\n",
    "Geometrically the dot product of two vectors is equal to multiply the lenght of the vectors by the cosine of the angle between them. \n",
    "\n",
    "<img src='pics/Cosine.png'>\n",
    "\n",
    "Remember that cosine is 1 if the angle is 0 and decreases the wider the angle becomes. \n",
    "\n",
    "This can be used as a similarity measure:\n",
    "\n",
    "<img src='pics/DotProductSimilarity.png'>\n",
    "\n",
    "If we have two vectors of same length, the smaller the angle between them the larger the dot product becomes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23032022\n",
    "\n",
    "Hidden state of the current timestep transposed times the matrix of the encoder:\n",
    "\n",
    "<img src='pics/MultiplicativeAttention.png'>\n",
    "\n",
    "This works assuming the decoder and encoder have the same embedding space. This is ok for text summarization (same language, same embedding) but for machine translation this would not work as each language would have its own embeddings spaces. \n",
    "\n",
    "For this case then we would want to use a slightly variated version of the above scoring method which simply introduces a weight matrix between encoder and decoder:\n",
    "\n",
    "<img src='pics/MultiplicativeAttentionWeightMatrix.PNG'>\n",
    "\n",
    "\n",
    "Below roughly the steps on how Attention would work:\n",
    "\n",
    "<img src='pics/MultiplicativeAttentionSteps.PNG'>\n",
    "\n",
    "Take initial hidden state and the embedding for the END symbol.\n",
    "\n",
    "It does the calculations and generates the hidden state H4 (in this case) at that timestep. Here we ignore the output of the RNN. Only hidden state.\n",
    "\n",
    "Then we do the Attention Step\n",
    "We take the matrix of the hidden states of the encoder, we produce a scoring, dot or general for example\n",
    "Scores are produced (red blocks)\n",
    "We do a softmax\n",
    "We multiply the softmax scores by each corresponding hidden state of the encoder.\n",
    "We sum them up producing the attention context vectors(Blue)\n",
    "\n",
    "Next we concatenate this  context vector with the hiddenstate\n",
    "We pass this through a fully connected NN, which is basically done by multiplying with the matric W_c\n",
    "We apply a tanh activation function\n",
    "\n",
    "The output of this is the first outputted word. \n",
    "\n",
    "For the next step we pass the hidden state and also the just outputted word.\n",
    "\n",
    "\n",
    "## Attention Scoring § Concat\n",
    "\n",
    "We fist concatenate the encoder hidden state and the hidden state at a specific timestep of the decoder. \n",
    "We pass this merged vector to the NN which has one hidden layer and outputs the score.\n",
    "The Merged vector is multiplied by a matrix of weights W_a (learnt during the training), the results are passed to a tanh which are then multiplied by another V_a matrix.\n",
    "\n",
    "<img src='pics/ConcatScoring.png'>\n",
    "\n",
    "Example of how attention works for image captioning:\n",
    "\n",
    "<img src='pics/Attention_ImageCaptioning.png'>\n",
    "\n",
    "<img src='pics/Attention_ImageCaptioning2.png'>\n",
    "\n",
    "\n",
    "## TRANSFORMERS \n",
    "\n",
    "A very famous paper published in 2017 called Attention is all you need, noted that the complexity of encoder/decoder with attention models could be simplified by adopting a new type of models that works only with Attention and no RNNs. This model is called Transformer. \n",
    "\n",
    "\n",
    "The transformer takes a sequence as an input and generate a sequence as an output. However, it takes everything at once and not one by one.\n",
    "\n",
    "Also the transformer model breaks down into an encoder and a decoder. \n",
    "Instead of using RNNs, they use feed -forward neural network in a concept called self attention. \n",
    "No RNNs.\n",
    "\n",
    "Transformers have a stack of identical encoder and decoder.\n",
    "\n",
    "<img src='pics/Transformer_Encoder_Decoder_Stack.PNG'>\n",
    "\n",
    "6 is the number the paper proposes.\n",
    "\n",
    "Each encoder layer contains two sublayers: Feed-forward and a Self-attention\n",
    "\n",
    "<img src='pics/Transformer_Encoder.png'>\n",
    "\n",
    "Multiheaded self attention. This is on the encoder side. This helps the encoder to focus on other parts of the input sequence.\n",
    "\n",
    "The deconder instead has the below structure:\n",
    "\n",
    "<img src='pics/Transformer_Dencoder.PNG'>\n",
    "\n",
    "Here there are to attention components. One that allows to focus on the relevant part of the input and one that pays attention to previous decoders outputs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 26/03/2022 GANs\n",
    "\n",
    "Generative Advesarial Networks, generally abbreviated as GANs, are neural networks that can create entirely new images that do not exist. \n",
    "\n",
    "GANs are used to generate realistic data, mostly used for images. Example of usages:\n",
    "\n",
    "\n",
    "### STACKGAN Model:\n",
    "\n",
    "This model is able to reproduce from a few line of text realistic images never seen before and imaginary. \n",
    "\n",
    "<img src='pics/StackGanExample.png'>\n",
    "\n",
    "### iGAN:\n",
    "\n",
    "Pix2Pix where you can drat a sketch and the model is able to output a realistic image based on it. \n",
    "\n",
    "<img src='pics/iGANExample.png'>\n",
    "\n",
    "GANs can also be used for image to image translation. For example a blueprint of a building can be turned into a realistic implementation of it or a sketch of a cat turned into a realistic cat:\n",
    "\n",
    "<img src='pics/GAN_Cat.png'>\n",
    "\n",
    "Image to Image translation can be trained unsupervised. For example in Meta AI, they trained a model on faces and cartoonish faces and there was no need for pairs.\n",
    "\n",
    "<img src='pics/ImageToImageUnsupervised.png'>\n",
    "\n",
    "A model that is good at image to image translation is CycleGAN, example:\n",
    "\n",
    "<img src='pics/CycleGAN.png'>\n",
    "\n",
    "Here the model transformed a horse into a zebra but it is so good at detecting that it is a different animal that it changed also the background, closer to a zebra. The grass looks more like an African grass. \n",
    "\n",
    "GANs can also be used to create Simulated Training Set, for example Apple used GANs to create realistic eyes from 3D printed eyes to train a computer vision model to detect where the eyes where staring, what direction:\n",
    "\n",
    "\n",
    "<img src='pics/SimulatedTrainingSet.png'>\n",
    "\n",
    "\n",
    "## How GANs work\n",
    "\n",
    "GANs are a kind of generative model that let's us generate a whole image in parallel. \n",
    "GANs use a differentiable function represented by a neural network as a generator network (G)\n",
    "\n",
    "<img src='pics/GANs.png'>\n",
    "\n",
    "The Generator takes in input random noise as input then runs that noise through a differentiable function to transform the noise and reshape it to a recongnizible structure. \n",
    "The output of a generator network is a realistic image. \n",
    "Running the generator with many different input noise values produces many realistic output images. \n",
    "The goal is for these images to be fair samples from the distribution over real data. \n",
    "The generator does not start out outputting realistic images. It needs to train to be able to generate them. \n",
    "\n",
    "The training process is much different than a supervised model where we show for example an image of a traffic light and we tell the model that this is a traffic light. \n",
    "\n",
    "For GANs there is no output associated with each image. \n",
    "Most generative models are trained by adjusting the parameters to maximize the probability that the generator net will generate the training data set. \n",
    "For many models this can be very difficult to compute this probability and get around it with some kind of approximation. \n",
    "\n",
    "GANs use an approximation called Discriminator (D) which is a second network that learns to guide the Generator. \n",
    "The Discriminator is a regular NN classifier.\n",
    "During the training process the Discriminator is shown real image from the training data half the time and half the time fake ones from the Generator.\n",
    "The Discriminator is trained to output the probability that the input is real. \n",
    "It assigns a probability near 1 to real images and near 0 to fake images. \n",
    "Meanwhile the Generator tries to do the opposite, it will try to generate images that the descriminator assigned probability close to 1 of being real. \n",
    "\n",
    "Overtime the Generator is forced to generate more realistic outputs in order to fool the descriminator. \n",
    "The generator takes random noise values Z and maps them to output X. \n",
    "\n",
    "<img src=\"pics/GANs_Probability.png\">\n",
    "\n",
    "Wherever the generator maps more values of Z, the probability distribution over X rapresented by the model becomes more dense. \n",
    "The descriminator outputs high values wherever the density of real data is greater than the density of generated data. \n",
    "\n",
    "In other words, the generators moves its samples into areas where the model distribution is not yet dense enough. \n",
    "Eventually the generators's distribution matches the real distribution.\n",
    "\n",
    "We can think of this process as being like a competiton between counterfeiters and police where the Generator is the group of counterfeiters and the Discrimator is the Police.\n",
    "The group of counterfeiters will try to produce fake money and pass it off as real. \n",
    "The police will try to identify if the money is fake or real. Overtime the police will get better at detecting fake money but so also the counterfeiters will get better at faking. \n",
    "Eventually the counterfeiters are forced to make perfect replicas of real money. \n",
    "\n",
    "\n",
    "## Training a GAN\n",
    "\n",
    "It is fundamental to choose an overall good architecture for the GAN. \n",
    "For a very simple task like generating small pics 28 by 28 from the MNIST dataset, a fully connected network will work. \n",
    "\n",
    "<img src='pics/GAN_architecture_for_MNIST.png'>\n",
    "\n",
    "The important thing is that both generator and descriminator have at least 1 hidden layer. \n",
    "For the hidden units many activation functions will work but LeakyReLU are popular as it makes sure that the gradien can flow through the entire architecture. \n",
    "The is more important for GANs as the only way for the generator to learn is to receive a gradient from the discriminator. \n",
    "A popular choice for the output of the Generator is tanH, so a value between -1 and 1.\n",
    "For most versions of GANs the output of the discriminator is a Sigmoid (0, 1) as it needs to be a probability. \n",
    "\n",
    "GANs require two optimization algorithms that run simultaneously. \n",
    "We define a loss for the generator and the discriminator, then we pass this to an optimizer, normally Adam (as in DCGAN) to minimize the loss for both the generator and the discriminator.\n",
    "\n",
    "To set up the discriminator loss, we need to remember that we want the discriminator to classify the images passed to it, if real or fake (1 or 0). This is just a Binary Classification task.\n",
    "For the loss we can use as criterion the nn.BCELoss(D_out, labels). BCE = Binary Cross Entropy. \n",
    "\n",
    "One place where often people make a mistake is they forget to use the numerical stable version of Cross Entropy where the loss is computer using the logits. \n",
    "\n",
    "<img src='pics/NumericalStableCrossEntropy.png'>\n",
    "\n",
    "The logits are the values produced by the discriminator right before the sigmoid. If you use the probability values that come out of the sigmoid there can be rounding issues when the sigmoid is near 0 or 1. \n",
    "In this case, for GANs, there is a trick to multiply the zero or one labels by a number that is just a little smaller than 1, e.g. 0.9 as pointed in the picture above. This is a label smoothing strategy to regularize normal classifiers. \n",
    "It helps the discriminator to generalize better and avoid learning to make extreme predictions when extrapolating. \n",
    "\n",
    "For the generator loss you want to set up another cross entropy loss but with the labeles flipped.\n",
    "\n",
    "<img src='pics/Loss_Generator.png'>\n",
    "\n",
    "The generator will maximixe the log probability of the wrong labels. \n",
    "\n",
    "Many people use negative d_loss for g_loss which is very intuitive as it corresponds to having the generator maximize the loss for the discriminator. \n",
    "However this does not work very well in practice and it is not recommended to implement it. \n",
    "This is because the gradient of d-loss is zero whenever the discriminator is winning. \n",
    "Using negative d-loss as g-loss foreces the generator to maximize cross-entropy while we really want both the discriminator and generator to minimize the cross-entropy.\n",
    "\n",
    "To scale GANs up to larger images we can adapt a similar architecture where convoluational layers are used:\n",
    "\n",
    "\n",
    "<img src='pics/GANs_with_CNN.png'>\n",
    "\n",
    "The input will be the vector Z which has to through a reshape as the CNN expects a 4D Tensor with one axis for different examples in the mini-batch, one axis for the different features maps and then the width and height axes. \n",
    "\n",
    "Normally with CNN, the image in input is a very tall and wide image with 3 feature maps (Red, Blue and Green color channels)\n",
    "When going thorugh Convolutional layers and pooling, the image ends up being a short and narrow feature maps.\n",
    "\n",
    "<img src='pics/CNN_input.png'>\n",
    "\n",
    "This is when we use a CNN as a classifier.\n",
    "\n",
    "If we use a CNN as a generator net, most researchers think we should do the opposite.\n",
    "We want to start off with a small feature map and expend it to a wide and tall image. \n",
    "\n",
    "<img src='pics/CNN_input_Generator.png'>\n",
    "\n",
    "The DCGAN project introducted the idea of increasing the size of the feature maps just by using a convolution transpose op with stride greater than 1. \n",
    "\n",
    "This means that every time we move the convolution kernel by 1 pixel, in the input map we move by 2 or some other larger number of pixels in the output map. \n",
    "\n",
    "Finally you really want to use batch normalization or one of the follow-up methods based on batch normalization in most of the layers of the network.\n",
    "The DCGAN authors suggest to use it on all layers except for the output layer of the generator and input layer of the discriminator.\n",
    "\n",
    "So to recap:\n",
    "\n",
    "- Convolutional Transpose\n",
    "- Adam\n",
    "- Label Smoothing and Cross Entropy\n",
    "- Batch Normalization\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## DCGAN\n",
    "\n",
    "Deep Convolutional Generative Adversarial Networks is made by a generator and discriminator. \n",
    "\n",
    "[DXGAN original paper](https://arxiv.org/pdf/1511.06434.pdf)\n",
    "\n",
    "The Discriminator is a convolutional neural network which has at the end a fully connect layer that will categorize 1 for real 0 for fake. \n",
    "\n",
    "<img src='pics/DCGAN_Discriminator.png'>\n",
    "\n",
    "***No maxpooling layers!***\n",
    "\n",
    "\n",
    "Only convolutional layers with stride = 2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pics/DCGAN_Stride2.png'>\n",
    "\n",
    "For each 2 pixel in the input img, there will be 1 pixel in the conv layer. This will downnsampling by a factor of 2.\n",
    "All hidden layers have batch normalization and leaky ReLU applied to the output. \n",
    "\n",
    "A Leaky ReLU function will reduce any negative values it sees by multiplying those values by a small coefficient called negative slope. \n",
    "\n",
    "Batch Normalization scaled the layers output to have a Mean = 0 and Variance = 1. Thanks to this, the training of the network is faster and more efficient and reduces issues with poor parameter initialization.  \n",
    "\n",
    "<img src='pics/DCGAN_Discriminator_Architecture.PNG'>\n",
    "\n",
    "\n",
    "For the Generator instead, we have Transpose Convolutional Layers.\n",
    "\n",
    "<img src='pics/Transpose_Conv.PNG'>\n",
    "\n",
    "In this case when passing through the convolutional layer, the input will double in size. \n",
    "\n",
    "<img src='pics/DCGAN_Input_Output.png'>\n",
    "\n",
    "In the Generator we first need to connect the input Z vector to a fully connected layer and then reshap it to a 4 by 4 with depth 512 \n",
    "\n",
    "<img src='pics/DCGAN_Generator.PNG'>\n",
    "\n",
    "The x, y dimensions are doubling for each layer but the depth is half each time. \n",
    "\n",
    "<img src='pics/DCGAN_Generator_Architecture.PNG'>\n",
    "\n",
    "***We use batch normalization and ReLu on all hidden layers except the last out where we use tanh.***\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9244e6e8c562ea0ec0726a39210e7e093201dbedd1fa49bbd40fbf29c80fe905"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
