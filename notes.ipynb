{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#14012022\n",
    "\n",
    "Concepts reviewed:\n",
    "\n",
    "Training and Testing Sets = Training model and use Testing to verify how good model performs. You never use testing data to train the model. \n",
    "Overfitting and Underfitting => Underfitting means the model is too simplistic and tend to oversimplifed the problem. Called also error due to bias. \n",
    "Overfittin is the opposite. Model is way too specific and performs pefectly on training set but on testing it wil fail as it cannot generalize that well. Called also error due to variance. \n",
    "Early Stopping => We do gradient descent until the testing error stops and starts increasing. That is the right spot and called early stopping. \n",
    "Regularization (L1 and L2) => T heproblem is that large coefficients are leading to it.We want to penalize in our error function large weights by either having a sum of the absolute values of the weights multiplied by lambda or the sum of the squares mulitplied by lambda. \n",
    "The first approach is called L1 Regularization. L1 tends to result in sparse vectors means that small weights tend to go to zero. Also this is good if we want to reduce the number of weights and have a small set. For feature selection is good because we can choose the most important ones and turn the rest to zero.\n",
    "The second approach is called L2 Regularization and it is good for training purposes. \n",
    "L2 does not favor sparse vectors since it tries to mantain all the weights homogeneously small. \n",
    "\n",
    "Dropout => a parameter given when training. e.g. 0.2 means 20% of the nodes will be turned off at each epoch. Some nodes may be never turned off and some more than others but in average each node will be turned off equally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#15012022\n",
    "\n",
    "Local Minima => When there are multiple low points and gradient descent may end up not at the lowest point but the lowest local point.\n",
    "To avoid this we can do a random restart from different positions and do gradient descent from all of them. \n",
    "Momentum => Another way to avoid local minima. The general idea is that the step to take will be an average of previous steps. Previous step multiplied by 1, previous one by Beta, previous one By Beta squared etc.. \n",
    "Vanish gradient problem => The sigmoid function gets pretty flat on the sides, so when calculating the derivatives at a point fat to right or left, the derivative is almost zero. \n",
    "This is a problem because the derivative is what tells you where to move towards and it would be very tiny steps.\n",
    "\n",
    "Hyperbolic Tangent Function => Another activation function. e^x - e^-x/e^x+ e^x.\n",
    "The values returned are between -1 and 1. \n",
    "\n",
    "ReLU => Rectified Linear Unit. Simple activation function. It return x if x is positive, if x is negative returns 0. ReLU is often used instead of Sigmoid. \n",
    "\n",
    "Stochastic gradient descent => If the data is well-distributed we can subset the data and run each subset throught the network, calculate the error and backpropagate. Then you repeat for next subset. \n",
    "Each step (epoch) may be less accurate than running once with all the data but in practice it is better to take a bunch of slightly inaccurate steps than take only one good one. \n",
    "\n",
    "Learning Rate => If learning rate is too big, you take too big steps. Too low learning rate will lead to too slow model and training. \n",
    "As a rule of thumb if the model is not working, you should decrease the learning rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#19112022\n",
    "\n",
    "Better initialization of the weights => In Sentiment Analysis prob 3, weights for the hidden layer to output were initialized as ```self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5, (self.hidden_nodes, self.output_nodes))```\n",
    "\n",
    "However, there is a better way of doing it. Normally the weights when initialized should be set to be close to zero without being too small.\n",
    "Good practice is to start the weights in the range of [-y, y] where y is 1/‚àön where n is the number of inputs of the layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#25012022\n",
    "\n",
    "`Softmax =>` The softmax function is used to calculate probabilities. This function can be used in a classifier only when the classes are mutually exclusive. \n",
    "The values returned will sum up to 1. \n",
    "<img src=\"pics/softmax_function.png\">\n",
    "\n",
    "<img src=\"pics/softmax_symbols.png\">\n",
    "\n",
    "\n",
    "The term on the bottom of the formula is the normalization term. It ensures that all the output values of the function will sum to 1 and each be in the range (0, 1), thus constituting a valid probability distribution.\n",
    "\n",
    "Simply put it you do the exponent (e^n) for each element in the vector, the sum of the value is the denominator. Then you divide each element of the input vector by the denominator and that will give you the probability. All of them summed up will result to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#26012022\n",
    "\n",
    "Loss function is used to be able to adjust the weight when backpropagation is done. The loss function calculates how bad the predictions were. A typical loss function is the mean squared loss.\n",
    "\n",
    "$$\n",
    "\\large \\ell = \\frac{1}{2n}\\sum_i^n{\\left(y_i - \\hat{y}_i\\right)^2}\n",
    "$$\n",
    "\n",
    "where $n$ is the number of training examples, $y_i$ are the true labels, and $\\hat{y}_i$ are the predicted labels.\n",
    "\n",
    "\n",
    "For a classification problem such the MNIST were a softmax is used to get the probabilities, the CrossEntropyLoss is the function to be used for calculating the loss. \n",
    "However with the CrossEntropyLoss in Pytorch you need to feed the raw output and not the output of the softmax function.\n",
    "\n",
    "Alternatively you can build the model with nn.LogSoftMax or F.log_softmax. Probabilities can be then obtained by doing the exponential (torch.exp(output)). With log_softmax you would need to use the NLLLoss (Negative likelihood loss).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#29012022\n",
    "\n",
    "Inherence is the process of making predictions.\n",
    "\n",
    "Pytorch notes:\n",
    "\n",
    "model.eval() is used to turn off dropout when doing validation\n",
    "Once one epoch is done, turn back on the dropout by using model.trai(). Always remember to do the whole validation with no_grad. \n",
    "\n",
    "To save and load a model in Pytorch it is important to know that when loading it back to another model, it will need to have the same architecture of the one saved.\n",
    "When saving the model, you can create a checkpoint dictionary:\n",
    "\n",
    "```\n",
    "checkpoint = {'input_size': 784,\n",
    "              'output_size': 10,\n",
    "              'hidden_layers': [each.out_features for each in model.hidden_layers],\n",
    "              'state_dict': model.state_dict()}\n",
    "```\n",
    "then save the model\n",
    "torch.save(checkpoint, 'checkpoint.pth')\n",
    "\n",
    "To load it back you can create a custom function to do that. Example:\n",
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath) => Load the pth file\n",
    "    model = fc_model.Network(checkpoint['input_size'], => Create the model based on the checkpoint content\n",
    "                             checkpoint['output_size'],\n",
    "                             checkpoint['hidden_layers'])\n",
    "    model.load_state_dict(checkpoint['state_dict']) => Load state dict\n",
    "    \n",
    "    return model\n",
    "    \n",
    "\n",
    "Then you can just simply do:\n",
    "\n",
    "model = load_state_dict('checkpoint.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#30012022\n",
    "\n",
    "Convolutional Neural Networks CNN\n",
    "\n",
    "Data Normalization is a pre-processing step which ensure that each input (in case of images), so each pixel is coming with a standard distribution. For example in MNIST hand-written numbers dataset the values of the pixelf would be from 0 to 255 (Black and white pics). However since the net workds with gradients we normalize the data to be between 0 and 1.\n",
    "\n",
    "Data normalization is typically done by substracting the mean of the pixel values from each pixel and then dividing the result by the standard deviation of all the pixel values. \n",
    "\n",
    "The distribution of this data will resemble of a Gaussian function. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#31012022\n",
    "\n",
    "CrossEntropyLoss in Pytorch has two steps, softmax and negative log likelihood loss.\n",
    "If using CrossEntropyLoss as criterion, then you do not need to specify the softmax as output layer.\n",
    "However, it is possible to alternatively define a log softmax layer in the model architecture and then use the NLLLoss as criterion. \n",
    "\n",
    "When building a model, we should always use 3 dataset, training, validation and testing.\n",
    "The validation is used while training but it is used to verify how well the model can generalize. With the validation set we do not update weights.\n",
    "The number of epochs to be used should be when the validation loss starts increasing and training loss keeps decreasing. That is when the model starts overfitting. \n",
    "The data for test is data has never been seen before by tbe model and should be used to verify how accurate the model is. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#01022022\n",
    "\n",
    "Differences between MLPs and CNNs\n",
    "\n",
    "MLPs use only fully connected layers while CNNs also use sparsely connected layers. In MLPs there is a lot of parameters and even with relatively small picture this number is high, resulting in higher computational needs. \n",
    "\n",
    "MLPs only accepts vectors (flattening the image) while CNNs also accept matrices. The fact of flattening an image can lose some information about the position of the pixels within the image.\n",
    "\n",
    "\n",
    "Convolutional Layer = Convolutional Kernel\n",
    "\n",
    "Spatial patterns in an image are related to either color or shape.\n",
    "\n",
    "Filters: to detect changes in intensity in an image, we use filters that look at groups of pixels and react to alternative patterns of dark/light pixels.These filters produce an output that shows edges of an object and differing texture. \n",
    "\n",
    "In images there is a rate of change. Similarly to audio there is high frequency and low frequency. \n",
    "\n",
    "Most images have both high-frequency and low-frequency components. high-frequency image pattern; this part changes very rapidly from one brightness to another. Instead where there is a  change very gradually, which is considered a smooth, low-frequency pattern.\n",
    "\n",
    "<img src='pics/high_low.png'>\n",
    "\n",
    "\n",
    "High pass  filters are applied to images to detect edges. Edges are areas in image where the intensity changes very quickly and very often indicates object boundaries. \n",
    "\n",
    "<img src='pics/Edge_detection_filter.png'>\n",
    "The sum of the matric has to be zero. If the result is positive the image will get brighter, if negative darker. Note that the values in orange summed are the value in the middle. \n",
    "<img src='pics/Convolutional_kernel.png'>\n",
    "\n",
    "Convolution is represented by asteriks, not multiplication\n",
    "\n",
    "<img src='pics/Convolutional_kernel_applied.png'>\n",
    "\n",
    "To find and enhancing an horizontal edge and line you would want to use for example the below kernel:\n",
    "\n",
    "-1 -2 -1\n",
    " 0  0  0\n",
    " 1  2  1\n",
    "\n",
    " This way you can see the difference between bottom and top\n",
    "\n",
    " Sobel filter is commonly used for edge detection and finding patterns in an image. Using the Sobel filter to an image is a way to taking an approximation of the derivative of the image in the x or y direction. \n",
    "\n",
    "\n",
    "Sx = [[-1 0 1],\n",
    "      [-2 0 2],\n",
    "      [-1 0 1]]\n",
    "\n",
    "Sy = [[-1  -2  -1],\n",
    "      [ 0   0   0],\n",
    "      [ 1   2   1]]\n",
    "\n",
    "\n",
    "\n",
    "A basic CNNs has the below structure:\n",
    "\n",
    "<img src='pics/CNN.png'>\n",
    "\n",
    "An image is passed through a filter, in this case 4 convolutional kernels which produce four differently filtered images which are then stacked together. This is the convolutional layer. \n",
    "\n",
    "<img src='pics/CNN2.png'>\n",
    "<img src='pics/CNN3.png'>\n",
    "\n",
    "Grey scale picture are 2D array, height and width.\n",
    "Colored pictures (RGB) are 3D array, height, width and depth. For RGB the depth is 3. \n",
    "A stack of 3 2D matrices. \n",
    "\n",
    "\n",
    "If you take a picture and apply 4 filters, both for detecting horizontal and vertical edges, there will be 4 collections of nodes, called also as feature maps or activation maps:\n",
    "\n",
    "<img src='pics/FeatureMaps.png'>\n",
    "\n",
    "<img src='pics/FeatureMaps2.png'>\n",
    "\n",
    "Note that in the picture above you can see the first two filters are discovering vertical edges. The first one has lighter features on the right, the second on the left. \n",
    "\n",
    "Edges and images appear as a line of lighter pixel next to darker pixels. \n",
    "\n",
    "Also on colored picture you apply filters which in this case are 3D as well, a stack of 3 2D matrices.\n",
    "\n",
    "<img src='pics/RGB_Convolutional.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#02022022\n",
    "\n",
    "Stride is how many pixel the filters will move when applied. For example if strid is 1, it will move 1 pixel at a time. \n",
    "When applying bigger strides you may end up having the filter going over the boundary of the image. In this case you could either ignore those regions and losing part of the information from the image or using padding so , adding 0s to the regions that were not covered by the filter. \n",
    "\n",
    "FILTER APPLIED (2x2):\n",
    "<img src='pics/Padding.png'>\n",
    "\n",
    "Ignore the regions not covered by the filter:\n",
    "\n",
    "<img src='pics/Ignore.png'>\n",
    "\n",
    "Or you could apply a padding by adding zeroes so that you retain all the information:\n",
    "\n",
    "<img src='pics/Padding2.png'>\n",
    "\n",
    "\n",
    "Pooling Layers:\n",
    "\n",
    "Pooling is the layer that takes in aa input a convolutional layer and returns as output the same number of feature maps but reduced. \n",
    "There are different pooling, e.g. max pooling and avg pooling. \n",
    "\n",
    "In Max pooling, once assigned a window size and a stride, the pooling will select the max number within the window:\n",
    "\n",
    "1 9 6 4\n",
    "5 4 7 8\n",
    "5 1 2 9\n",
    "6 7 6 0\n",
    "\n",
    "You start from left top and you would have\n",
    "1 9\n",
    "5 4\n",
    "\n",
    "In this case it takes 9\n",
    "\n",
    "Stride is 2 so next will be\n",
    "6 5\n",
    "7 8\n",
    "\n",
    "In this case take 8\n",
    "\n",
    "Repeat this and you get\n",
    "\n",
    "9 8\n",
    "7 9\n",
    "\n",
    "So from a 4x4 it is now a 2x2. \n",
    "\n",
    "The average pooling instead of return an avg of the window size instead of the max. However in image classification, max pooling is more effective and notices better edges, without missing details. \n",
    "\n",
    "<img src='pics/MaxPooling.png'>\n",
    "\n",
    "<img src='pics/MaxPooling2.png'>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#03022022\n",
    "\n",
    "Remember that pooling throws away some information of an image which may cause some issues, for example in Face recognition.\n",
    "An alternative to pooling is Capsule Networks. Capsule Networks provide a way to detect parts of objects and represent spatial relationships between them. Capsule networks are made of parent and child nodes that build up a complete picture of an object.\n",
    "\n",
    "<img src=\"pics/Capsule_Net.png\">\n",
    "\n",
    "Capsule are basically a collection of nodes, each one containing information about a specific part of the object, e.g. width, orientation, color etc.\n",
    "\n",
    "Each capsule outputs a vector with some magnitude and orientation:\n",
    "\n",
    "Magnitude (m) is the probability that a part exists, a value between 0 and 1.\n",
    "Orientation (Œ∏, theta) is the state of the part properties. \n",
    "To check this repo: https://github.com/cezannec/capsule_net_pytorch\n",
    "\n",
    "\n",
    "Depth: Colored pics have 3, Black and white 1. \n",
    "When working with images, we need to resize them before doing anything. The resizing typically means means to resize the pics as a square. \n",
    "All pics have to have a fixed size also for CNNs with spatial dimension equal to a power of 2 or else a number that is divisible by a large power of 2.\n",
    "\n",
    "How to define a conv layer in Pytorch, example:\n",
    "\n",
    "self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "First 3 is the depth of the image, so if a color image it will be 3\n",
    "16 is for how many filters we want to have\n",
    "The second 3 is the kernel_size, so in this case a 3x3. Typically it can be from 2x2 to 7x7 or up for bigger pics.\n",
    "padding=1 is to make sure we do not loose information from the borders of the pics\n",
    "\n",
    "<img src=\"pics/Conv_Layers.png\">\n",
    "\n",
    "Note that for each conv layer, the number of filters is doubling, and so is the depth. Depth is the filter number of the prev layer.\n",
    "\n",
    "Next is the addition of Pooling so that the dimensions of x, y get cut in half in the case of using a kernel and stride of 2 and 2:\n",
    "\n",
    "<img src=\"pics/PyTorch_MaxPooling.png\">\n",
    "\n",
    "QUESTIONS: \n",
    "\n",
    "How might you define a Maxpooling layer, such that it down-samples an input by a factor of 4? \n",
    "nn.MaxPool2d(4,4) or nn.MaxPool2d(2,4). \n",
    "Since the stride is 4 you would downsample by 4 but it is best to go for 4,4 so that the maxpooling function sees every input pixel once.\n",
    "\n",
    "If you want to define a convolutional layer that is the same x-y size as an input array, what padding should you have for a kernel_size of 7? (You may assume that other parameters are left as their default values.)\n",
    "\n",
    "padding=3\n",
    "\n",
    "E.g. if your pic is 32x32 and kernel is 7x7, after 4 iterations, we are at pixel 28, the kernel will overlay 3 columns that do not exist. \n",
    "\n",
    "nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)\n",
    "in_channels refers to the depth of an input. For a grayscale image, this depth = 1\n",
    "out_channels refers to the desired depth of the output, or the number of filtered images you want to get as output\n",
    "kernel_size is the size of your convolutional kernel (most commonly 3 for a 3x3 kernel)\n",
    "stride and padding have default values, but should be set depending on how large you want your output to be in the spatial dimensions x, y\n",
    "\n",
    "\n",
    "The number of parameters is the x-y size of the final output times the number of final channels/depth, so if pic is 16*16 and last conv layer was filter 20, then it is 16*16*20\n",
    "\n",
    "<img src=\"pics/CNN_Prediction.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#05022022\n",
    "\n",
    "To compute the output size of a convolutional layer we can obtain it from the below formula\n",
    "\n",
    "(W‚àíF+2P)/S+1\n",
    "\n",
    "So if you had a 7x7 input with a 3x3 kernel, padding 0 and stride of 1, the output would be a 5x5: ((7-3+0)/1)+1=5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#06022022\n",
    "\n",
    "When initializing weights for a model it makes sense to have them based on the amount of nodes we have in input. \n",
    "The more inputs a certain node sees the smaller the weight should be.\n",
    "\n",
    "Good practice is to start your weights in the range of  [‚àíùë¶,ùë¶]  where  ùë¶=1/sqr.root(n)  is the number of inputs to a given neuron)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#07022022\n",
    "\n",
    "Autoencoder: you can think the process of an image going through convolutional layers as a sort of decompression. Autoencoders compress and decompress data. \n",
    "\n",
    "<img src='pics/Autoencoders.png'>\n",
    "\n",
    "An autoencoder has two parts. Encoder that compresses the data and a decoder that reconstruct the data from the compressed one. This is also used in for example GANs.\n",
    "They work well for de-noising and also filling in missing data. \n",
    "\n",
    "<img src='pics/DenoiseImageTrans.png'>\n",
    "\n",
    "The middle layer is the compressed representation of the input from which you can reconstruct the data. Key aspect of an autoencoders is that it can compresse the data such that the content is still maintained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#08022022\n",
    "\n",
    "A simple MLP autoencoder does the job on images like MNIST but a better solution is to use convolutional neural net. \n",
    "The first portion, the encoder, is a stack of convolutional layers and pooling. The second portion, the decoder should reverse the process done by the encoder, it should upscale the image. \n",
    "One way of doing this is called nearest neighbor where pixels are replicated, for example:\n",
    "\n",
    "1 2           1 1 2 2\n",
    "3 4           1 1 2 2\n",
    "              3 3 4 4 \n",
    "\n",
    "However this is just duplicating values and in large images, there are more details and this approach would not work. \n",
    "A better solution to this is transponse convolutional layer.\n",
    "It is important to keep in mind that the stride of the kernel is approximately the ratio of input to ouput dimensions. \n",
    "<img src='pics/StrideRatio.png'>\n",
    "\n",
    "The transpose takes for example the first pixel and places a kernel of 3x3 on it, multiplies that pixel by the kernel weights to get a 3x3 area:\n",
    "<img src='pics/MultipliedKernel.png'>\n",
    "\n",
    "Then with stride 2 we know that the center of the second pixel is 2 positions to the right:\n",
    "\n",
    "<img src='pics/SecondPixel.png'>\n",
    "\n",
    "So the two areas will overlap and the area with overlapping values will be summed up:\n",
    "\n",
    "<img src='pics/SumOverlap.png'>\n",
    "\n",
    "The final outputput would be after doing all 4 pixels a 5x5 area:\n",
    "\n",
    "<img src='pics/AutoencoderOutput.png'>\n",
    "\n",
    "Generally you would use a kernel of 2x2 and a stride of 2 to double the x, y dimensions. \n",
    "\n",
    "<img src='pics/Kernel2Stride2.png'>\n",
    "\n",
    "\n",
    "Good article to read about deconvolution: https://distill.pub/2016/deconv-checkerboard/\n",
    "\n",
    "It is important to note that transpose convolution layers can lead to artifacts in the final images, such as checkerboard patterns.\n",
    "These artifacts can be avoided by resizing the layers using nearest neighbor or bilinear interpolation (upsampling) followed by a convolutional layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#10022022\n",
    "\n",
    "Style Transfer is the ability to apply a style of one image to a content of another image. For example:\n",
    "\n",
    "<img src=\"pics/StyleTransfer.png\">\n",
    "\n",
    "An example for applying style transfer is with VGG19: https://arxiv.org/pdf/1409.1556.pdf\n",
    "\n",
    "The VGG19 CNN has 5 pooling layers and each convolutional layer is a stack of layers:\n",
    "\n",
    "<img src=\"pics/VGG19_ConvLayers.png\">\n",
    "\n",
    "To obtain the target image, the one with content from one image and style from another, we pass both content and style image through the VGG19.\n",
    "\n",
    "When passing the content image, the image will go through till the deepest conv layer, where a representation of the image will be the output. \n",
    "\n",
    "<img src=\"pics/Content_Representation.PNG\">\n",
    "\n",
    "Next we pass the style image and the network will extract different features from multiple layers that represent the style of that image.\n",
    "\n",
    "<img src=\"pics/StyleRepresentation.png\">\n",
    "\n",
    "The output will be the merging of content representation and style representation.\n",
    "\n",
    "The challenge is to get the target image which can start as a blank canvas or a copy of the input image and need to manipulate the style. \n",
    "\n",
    "For the content representation, in the paper, the output is takem from the conv layer 4_2. The output is then compared to the input image.\n",
    "\n",
    "<img src=\"pics/ContentRepresentationOutput.png\">\n",
    "\n",
    "We need to calculate a content loss which is the mean square loss between the content representation from image and content representation of the  target image.\n",
    "\n",
    "<img src=\"pics/ContentLoss.png\">\n",
    "\n",
    "This will measure how far apart are the two representation from each other. The aim is to minimize the loss by backpropagation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#12022022\n",
    "\n",
    "Similarly to comparing the content representation and target rapresentation, we do for the style where we need to compare the style representation of the style image and the target image. \n",
    "We basically check how similar features are in the same layer of the network. Similarities will be colors and textures.\n",
    "\n",
    "<img src='pics/MultiScaleRep.png'>\n",
    "\n",
    "By including the correlations of between multi layers of different sizes we can obtain a multi scale style representation of the style image where lareg and small style features are caught. \n",
    "\n",
    "The gram matrix defines the correlations. \n",
    "\n",
    "<img src='pics/grammatrix.png'>\n",
    "\n",
    "If we have a 4x4 pic passed through the conv layer with 8 of depth, the matrix will have 8 feature maps that we want to find the relationships between. \n",
    "\n",
    "We basically vectorize (flatten the values).First row in the feature map are the first 4 slot of the vector.\n",
    "\n",
    "<img src='pics/FlattenGram.png'>\n",
    "\n",
    "By vectorizing the feature maps we transfor a 3D conv layer into a 2D matrix of values \n",
    "\n",
    "<img src='pics/ConversionGram.png'>\n",
    "\n",
    "Next we need to multiply this matrix by the Transponse of the matrix\n",
    "\n",
    "<img src='pics/TransposeGram.PNG'>\n",
    "\n",
    "The result will be a 8 by 8 matrix:\n",
    "\n",
    "<img src='pics/FinalGram.png'>\n",
    "\n",
    "So for example the value in row 4, column 2, will hold the similarities of the fourth and second feature maps in the layer. \n",
    "Gram matrix is one of the mostly used in practice when doing style. \n",
    " \n",
    "\n",
    "\n",
    "QUESTIONS:\n",
    "\n",
    "Given a convolutional layer with dimensions d x h x w = (20*8*8), what length will one row of the vectorized convolutional layer have? (Vectorized means that the spatial dimensions are flattened.)\n",
    "\n",
    "64 => multiply h x w since it is a 2D vector\n",
    "\n",
    "\n",
    "Given a convolutional layer with dimensions d x h x w = (20*8*8), what dimensions (h x w) will the resultant Gram matrix have?\n",
    "\n",
    "20*20 => The gram matrix will have the number of rows and columns as the depth of the conv layer. \n",
    "\n",
    "Same as for content, the style loss is calculated. At each of the 5 layers by comparing target vs style image. We only change the target image.\n",
    "\n",
    "The total loss is basically the sum of the content loss and style loss. We reduce the loss by backpropagation.\n",
    "\n",
    "<img src='pics/TotalLoss.PNG'>\n",
    "\n",
    "However since the style loss and content loss are calculated differently we need to apply constant weights to each. Normally the constant used for style is much larger than the one for content. \n",
    "\n",
    "<img src='pics/ConstantsAlfaBeta.png'>\n",
    "\n",
    "Normally we talk about alfa over beta as ratio.\n",
    "Example of ratio: 1/10\n",
    "\n",
    "<img src='pics/ExampleRatio.png'>\n",
    "\n",
    "In this example we can see there is much of content but not a lot of style. When increasing the beta, the target image starts having more and more style. \n",
    "\n",
    "<img src='pics/IncreaseBeta.png'>\n",
    "\n",
    "Until a certain point the style is overclassing content. 10 to the -4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#15022022\n",
    "\n",
    "From Wikipedia:\n",
    "Precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances, while recall (also known as sensitivity) is the fraction of relevant instances that were retrieved. Both precision and recall are therefore based on relevance.\n",
    "\n",
    "Consider a computer program for recognizing dogs (the relevant element) in a digital photograph. Upon processing a picture which contains ten cats and twelve dogs, the program identifies eight dogs. Of the eight elements identified as dogs, only five actually are dogs (true positives), while the other three are cats (false positives). Seven dogs were missed (false negatives), and seven cats were correctly excluded (true negatives). The program's precision is then 5/8 (true positives / selected elements) while its recall is 5/12 (true positives / relevant elements).\n",
    "\n",
    "\n",
    "Sensitivity and specificity are not the same as precision and recall\n",
    "\n",
    "Sensitivity: Of all the people with cancer, how many were correctly diagnosed?\n",
    "\n",
    "Specificity: Of all the people without cancer, how many were correctly diagnosed?\n",
    "\n",
    "And precision and recall are the following:\n",
    "\n",
    "Recall: Of all the people who have cancer, how many did we diagnose as having cancer?\n",
    "Precision: Of all the people we diagnosed with cancer, how many actually had cancer?\n",
    "From here we can see that Sensitivity is Recall, and the other two are not the same thing.\n",
    "\n",
    "<img src='pics/confusion-matrix.png'>\n",
    "\n",
    "Sensitivity = {TP}/{TP + FN}\n",
    "‚Äã\n",
    "and\n",
    "\n",
    "Specificity = {TN}/{TN + FP}\n",
    "\n",
    "<img src='pics/precision-recall.png'>\n",
    "\n",
    "ROC Curves => Receiver Operating Characteristic curve is used to evaluate a model. \n",
    "\n",
    "<img src='pics/ROC.png'>\n",
    "\n",
    "When calculating and plotting the data you calculate the True Positive and False Positive Rate\n",
    "\n",
    "True Positive Rate = True Pos/All Pos\n",
    "False Positive Rate = False Pos/All Negative\n",
    "\n",
    "<img src='pics/TPFPRatio.png'>\n",
    "\n",
    "You can keep calculating at each split and get all coordinates and plot them to a graph. This will give you the ROC. \n",
    "\n",
    "Below an example of ROC for the skin cancer detection (note that it is flipped):\n",
    "\n",
    "<img src='pics/ROC_SkinCancer.png'>\n",
    "\n",
    "https://www.nature.com/articles/nature21056.epdf?author_access_token=8oxIcYWf5UNrNpHsUHd2StRgN0jAjWel9jnR3ZoTv0NXpMHRAJy8Qn10ys2O4tuPakXos4UhQAFZ750CsBNMMsISFHIKinKDMKjShCpHIlYPYUHhNzkn6pSnOCt0Ftf6\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#16022022\n",
    "\n",
    "Confusion Matrix:\n",
    "\n",
    "Model if patient is heatlhy or sick\n",
    "\n",
    "If sick and diagnosed sick => True Positive\n",
    "\n",
    "If Sick and Diagnosed Healthy => False Negative\n",
    "\n",
    "If Heatlhy and Diagnosed Sick => False Positive\n",
    "\n",
    "If Healthy and Diagnosed Healthy => True Negative\n",
    "\n",
    "Type 1 => False Positive (When heatlhy was diagnosed sick)\n",
    "\n",
    "Type 2 => False Negative (When sick was diagnosed heatlhy)\n",
    "\n",
    "In a confusion matrix, entries must be between 0 and 1. The sum of every row must be 1.\n",
    "The confusion matrix can be also more than just 2x2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#22022022\n",
    "\n",
    "Recurrent Neural Network or RNN are artificial neural network that have memory that can capture temporal dependencies which are dependencies over time.\n",
    "In neural networks like CNN there is a lack of memory while RNN uses memory when producing an output.\n",
    "\n",
    "Year 1989, The first RNN was the TDNN ot Time Delay Neural Network. Where inputs from past timesteps were introduced to the network input changing the actual external inputs allowing to look beyond the current timestep but also limited to the window selected. \n",
    "\n",
    "Year 1990, Simple RNN or Elman Network was introduced. However these RNN suffered from a flaw that goes by the name of vanishing gradient. Basically when training the network we do backpropagation which adjusts the weight matrices with the gradient. During this process we continuosly multiply derivaties and the values may be so small that the gradient will vanish. \n",
    "\n",
    "CHAIN RULE REMINDER\n",
    "\n",
    "<img src='pics/chainrule.png'>\n",
    "\n",
    "\n",
    "Backpropagation is just a calculation of partial derivatives.\n",
    "\n",
    "BACKPROPAGATION REMINDER\n",
    "\n",
    "What is the update rule of weight matrix W1. What is the partial derivative of y with respect to W1. \n",
    "\n",
    "<img src='pics/BackpropReminder.png'>\n",
    "<img src='pics/Derivatives.png'>\n",
    "\n",
    "C as there are two separate paths for which W1 contributes to:\n",
    "\n",
    "<img src='pics/Path1.png'>\n",
    "<img src='pics/Path2.png'>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***2602202***\n",
    "\n",
    "RNNs are based on similar principles as feedforward network (FFNs) however there are two main differences between FFNs and RNNs:\n",
    "\n",
    "Sequences as input in training phase and memory elements (States)\n",
    "\n",
    "Memory is defined as the output of hidden layer neurons which will be used as additional input to the network during next training step. \n",
    "\n",
    "The basic three layer neural network with feedback that serve as memory input is called simple RNN or Elman Network. \n",
    "\n",
    "<img src='pics/Elman.png'>\n",
    "\n",
    "<img src='pics/Unfolded.png'>\n",
    "\n",
    "In RNNs the output depends not only on the input of the current input and weights but also previous inputs, as shown above. \n",
    "\n",
    "<img src='pics/FoldedModel.png'>\n",
    "\n",
    "In FFNs the hidden layer depended solely on the current input and weights as well as an activation function \\PhiŒ¶\n",
    "In RNNs the state layer depends on the current inputs, weights, activation function and also on the previous state:\n",
    "\n",
    "<img src='pics/EquationRNN.png'>\n",
    "\n",
    "The ouput vector is calculate the same way as for FFNN which can be a linear combination of inputs to each output node with corresponding matrix Wy or a softmax function of the same combination.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9244e6e8c562ea0ec0726a39210e7e093201dbedd1fa49bbd40fbf29c80fe905"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
